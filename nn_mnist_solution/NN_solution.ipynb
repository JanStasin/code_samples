{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(r'mnist.pkl')\n",
    "train_d, val_d, test_d = data\n",
    "\n",
    "#training data\n",
    "train_data, train_labels = train_d\n",
    "train_data = train_data.T\n",
    "train_labels = train_labels.T\n",
    "#validation data\n",
    "val_data, val_labels = val_d\n",
    "val_data = val_data.T\n",
    "val_labels = val_labels.T\n",
    "# testing data\n",
    "test_data, test_labels = test_d\n",
    "test_data = test_data.T\n",
    "test_labels = test_labels.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z, derivative=False):\n",
    "    if derivative == True:\n",
    "        return sigmoid(Z)*(1-sigmoid(Z))\n",
    "    return 1.0 / (1.0 + (np.exp(-Z)))\n",
    "\n",
    "def ReLu(Z, derivative=False):\n",
    "    if derivative == True:\n",
    "        return Z > 0\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def SoftMax(Z):\n",
    "    '''computes softmax takes Z as an argument and returns probabilities '''\n",
    "    return np.exp(Z) / np.sum(np.exp(Z), axis=0)\n",
    "    \n",
    "def vectorize_labels(labels):\n",
    "    \"\"\"Converts the labels into into a vectors with a 1.0 in the respective label\n",
    "    position and zeroes elsewhere.  This is used further in cost function computations.\"\"\"\n",
    "    v = np.zeros((labels.size, max(labels) + 1))\n",
    "    v[np.arange(labels.size), labels] = 1.0\n",
    "    v = v.T\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_input = train_data[:,7]\n",
    "td0_reshaped = np.reshape(first_input, (28,28))\n",
    "first_output = train_labels[7]\n",
    "plt.imshow(td0_reshaped)\n",
    "plt.colorbar()\n",
    "print(first_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    \n",
    "    def __init__(self ,L_sizes):\n",
    "        '''network setup based on layer sizes array supplied at the beginning\n",
    "        each element of the array defines the number of nodes for each layer.\n",
    "        784 is the basic input size 10 is the basic output size (last element of the array)\n",
    "        we dont need to create activation values in advance they will be dynamically computed during feedforward \n",
    "        procedure '''\n",
    "        ## initializing weights and biases at each layer\n",
    "        self.W1 = np.random.rand(L_sizes[1], L_sizes[0])- 0.5\n",
    "        print('W1 shape: ', self.W1.shape)\n",
    "        self.W2 = np.random.rand(L_sizes[2], L_sizes[1])- 0.5\n",
    "        print('W2 shape: ', self.W2.shape)\n",
    "        self.W3 = np.random.rand(L_sizes[3], L_sizes[2]) - 0.5\n",
    "        print('W3 shape: ', self.W3.shape)\n",
    "        self.B1 = np.random.rand(L_sizes[1],1)- 0.5\n",
    "        print('B1 shape: ', self.B1.shape)\n",
    "        self.B2 = np.random.rand(L_sizes[2],1)- 0.5\n",
    "        print('B2 shape: ', self.B2.shape)\n",
    "        self.B3 = np.random.rand(L_sizes[3],1)- 0.5\n",
    "        print('B3 shape: ', self.B3.shape)\n",
    "    \n",
    "    \n",
    "    def forward_feed(self, data_input, a_func): ### can activation function be added dynamically?\n",
    "        '''runs the feed forward through the whole network\n",
    "        inputs: network state, input data and activation function\n",
    "        outputsL: zs and activations for each layer'''\n",
    "        z1= np.dot(self.W1, data_input) + self.B1 ## dot product of input matrix and weights matrix plus respective biase\n",
    "        A1= a_func(z1) ## application of activation function\n",
    "        \n",
    "        z2= np.dot(self.W2, A1) + self.B2 ## repeat for the next layers\n",
    "        A2= a_func(z2)\n",
    "        \n",
    "        z3= np.dot(self.W3, A2) + self.B3\n",
    "        #A3= sigmoid(z3)\n",
    "        A3= SoftMax(z3)\n",
    "        #print(A3)\n",
    "        return z1, z2, z3, A1, A2, A3;\n",
    "\n",
    "    def backprop_feed(self, z1, z2, z3, A1, A2, A3, data_input, labels, a_func):\n",
    "        '''runs the backpropagation through the whole network\n",
    "        inputs: network state, input data, and corresponding labels + activation function\n",
    "        outputsL: network state, changes to Weights and Biases for each layer'''\n",
    "        v_labels = vectorize_labels(labels)\n",
    "        \n",
    "        #output layer\n",
    "        d_z3 = A3 - v_labels #activation of the output layer - the vectorized label\n",
    "        d_W3 = 1/v_labels.size * np.dot(d_z3, A2.T) #dot prod of z3 and activations on the hidden layer 2 \n",
    "        d_B3 = 1/v_labels.size * np.sum(d_z3, 1)\n",
    "\n",
    "        # hidden layer 2:\n",
    "        d_z2 = np.dot(self.W3.T, d_z3) * a_func(z2, derivative = True)\n",
    "        d_W2 = 1/v_labels.size * np.dot(d_z2, A1.T) #dot prod of z2 and activations on the hidden layer 1 \n",
    "        d_B2 = 1/v_labels.size * np.sum(d_z2, 1)\n",
    "\n",
    "        # hidden layer 1:\n",
    "        d_z1 = np.dot(self.W2.T, d_z2) * a_func(z1, derivative = True)\n",
    "        d_W1 = 1/v_labels.size * np.dot(d_z1, data_input.T) #dot prod of z2 and activations on the input layer \n",
    "        d_B1 = 1/v_labels.size * np.sum(d_z1, 1)\n",
    "        return d_W1, d_B1, d_W2, d_B2, d_W3, d_B3 ### returning changes to weights and biases for updating\n",
    "    \n",
    "    def update(self, d_W1, d_B1, d_W2, d_B2, d_W3, d_B3, learning_rate):\n",
    "        '''This method updates the weights and biases and applies learning rate\n",
    "        Arguments: derivatives of weights and biases from backprop feed and learning rate'''\n",
    "        self.W1 = self.W1 - (learning_rate * d_W1)\n",
    "        self.B1 = self.B1 - (learning_rate * np.reshape(d_B1, (len(d_B1),1)))\n",
    "        self.W2 = self.W2 - (learning_rate * d_W2)\n",
    "        self.B2 = self.B2 - (learning_rate * np.reshape(d_B2, (len(d_B2),1)))\n",
    "        self.W3 = self.W3 - (learning_rate * d_W3)\n",
    "        self.B3 = self.B3 - (learning_rate * np.reshape(d_B3, (len(d_B3),1)))\n",
    "        return self.W1, self.B1, self.W2, self.B2, self.W3, self.B3\n",
    "   \n",
    "    def interpret_output(self, output_layer):\n",
    "        '''takes A3 - activations of the ouput layer and outputs the corresponding 0-9 integer'''\n",
    "        return np.argmax(output_layer, 0)\n",
    "    \n",
    "    def get_network_accuracy(self, guesses, labels):\n",
    "        '''as advertised: compares all network guesses against the corresponding labels\n",
    "        Arguments: guesses and labels \n",
    "        Output: Percentage correct - accuracy''' \n",
    "        print('guesses: ', guesses)\n",
    "        print('labels : ', labels)\n",
    "        return np.sum(guesses == labels) / labels.size\n",
    "    \n",
    "\n",
    "    def raw_gradient_descent(self, data_input, labels, a_func, learning_rate, iterations):\n",
    "        '''Runs a gradient decent algorythm on the whole input data set for the specified number of times\n",
    "        Arguments: data, corresponding labels, activation function\n",
    "        learning rate,  number of times to run the data through the algorythm\n",
    "        Output: state of the network'''\n",
    "        for i in range(iterations):\n",
    "            z1, z2, z3, A1, A2, A3 = self.forward_feed(data_input, a_func)\n",
    "            d_W1, d_B1, d_W2, d_B2, d_W3, d_B3 = self.backprop_feed(z1, z2, z3, A1, A2, A3,\n",
    "                                                                    data_input, labels, a_func)\n",
    "            self.W1, self.B1, self.W2, self.B2, self.W3, self.B3 = self.update(d_W1, d_B1, d_W2, d_B2,\n",
    "                                                                               d_W3, d_B3, learning_rate)\n",
    "            if i % 100 == 0:\n",
    "                print(\"Iteration: \", i)\n",
    "                guesses = self.interpret_output(A3)\n",
    "                #print('first A3', A3 )\n",
    "                print('Accuracy: ', self.get_network_accuracy(guesses, labels)) \n",
    "        print('Training accuracy: ', self.get_network_accuracy(guesses, labels)) \n",
    "        #return self.W1, self.B1, self.W2, self.B2, self.W3, self.B3\n",
    "        return self.get_network_accuracy(guesses, labels)\n",
    "\n",
    "    \n",
    "    def make_predictions(self, data_input, a_func):\n",
    "        '''function used for getting the predictions on the validation / testing datasets\n",
    "        applies interpret output function to the output layer after a forward feed with data\n",
    "        Arguments: data and activation function (used for training)'''\n",
    "        *_ , self.A3 = self.forward_feed(data_input, a_func)\n",
    "        predictions = self.interpret_output(self.A3)\n",
    "        return predictions\n",
    "\n",
    "    def test_prediction(self, index, a_func):\n",
    "        current_image = train_data[:, index, None]\n",
    "        prediction = self.make_predictions(train_data[:, index, None], a_func)\n",
    "        label = train_labels[index]\n",
    "        print(\"Prediction: \", prediction)\n",
    "        print(\"Label: \", label)\n",
    "\n",
    "        current_image = current_image.reshape((28, 28)) * 255\n",
    "        plt.gray()\n",
    "        plt.imshow(current_image)\n",
    "        plt.show()  \n",
    "        \n",
    "NN = Network([784, 16, 16 ,10])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "NN = Network([784, 16, 16 ,10])\n",
    "NN.raw_gradient_descent(train_data, train_labels , ReLu, 1, 500)\n",
    "print(time.time()-t1)\n",
    "\n",
    "# 100.0616, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.test_prediction(240, ReLu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions = NN.make_predictions(val_data[:, :], ReLu)\n",
    "NN.get_network_accuracy(val_predictions, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = NN.make_predictions(test_data[:, :], ReLu)\n",
    "NN.get_network_accuracy(test_predictions, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN = Network([784, 16, 16 ,10])\n",
    "# accs_relu = []\n",
    "# for it in range(8100):\n",
    "#     accs_relu.append(NN.raw_gradient_descent(train_data, train_labels , ReLu, 0.9, 1))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN2 = Network([784, 16, 16 ,10])\n",
    "# accs_sigm = []\n",
    "# for it in range(20000):\n",
    "#     accs_sigm.append(NN2.raw_gradient_descent(train_data, train_labels , sigmoid, 0.9, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs_sigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(4)    \n",
    "plt.plot(accs_relu, 'r')\n",
    "plt.plot(accs_sigm, 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(accs_relu) , max(accs_sigm))\n",
    "accs_relu[8070]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs_alfa = []\n",
    "alfa = np.arange(0.01 ,1,0.08)\n",
    "for a in alfa:\n",
    "    print(a)\n",
    "    NN3 = Network([784, 16, 16 ,10])\n",
    "    accs_alfa.append(NN3.raw_gradient_descent(train_data, train_labels , ReLu, a, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(4)    \n",
    "plt.plot(accs_alfa, 'k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs_hl1 = []\n",
    "for i, j in range(6,33):\n",
    "    print(i, j)\n",
    "    #NN3 = Network([784, i, 16 ,10])\n",
    "    #accs_alfa.append(NN3.raw_gradient_descent(train_data, train_labels , ReLu, a, 500))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
