{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6942c99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 19:15:50.199648: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# System and utilities\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import io\n",
    "import base64\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "# Data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Natural Language Processing (NLP)\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "import unicodedata\n",
    "import re\n",
    "import ast\n",
    "#import spacy\n",
    "#from langdetect import detect\n",
    "\n",
    "# Machine Learning & Modeling\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Image processing and display\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# # Custom modules\n",
    "# from auto_translate import AutoTranslate\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f097aada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84916, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>prdtypecode</th>\n",
       "      <th>cat_name_fr</th>\n",
       "      <th>cat_name_en</th>\n",
       "      <th>combined</th>\n",
       "      <th>comb_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3804725264</td>\n",
       "      <td>1263597046</td>\n",
       "      <td>10</td>\n",
       "      <td>Livres d'occasion</td>\n",
       "      <td>Used Books</td>\n",
       "      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n",
       "      <td>['olivia', 'personalisiertes', 'notizbuch', 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>436067568</td>\n",
       "      <td>1008141237</td>\n",
       "      <td>2280</td>\n",
       "      <td>Revues et Magazines</td>\n",
       "      <td>Magazines and Journals</td>\n",
       "      <td>Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...</td>\n",
       "      <td>['journal', 'arts', 'art', 'marche', 'salon', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n",
       "      <td>PILOT STYLE Touch Pen de marque Speedlink est ...</td>\n",
       "      <td>201115110</td>\n",
       "      <td>938777978</td>\n",
       "      <td>50</td>\n",
       "      <td>Accessoires de jeux vidéo</td>\n",
       "      <td>Video Game Accessories</td>\n",
       "      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n",
       "      <td>['stylet', 'ergonomique', 'gamepad', 'nintendo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         designation  \\\n",
       "0  Olivia: Personalisiertes Notizbuch / 150 Seite...   \n",
       "1  Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...   \n",
       "2  Grand Stylet Ergonomique Bleu Gamepad Nintendo...   \n",
       "\n",
       "                                         description   productid     imageid  \\\n",
       "0                                                NaN  3804725264  1263597046   \n",
       "1                                                NaN   436067568  1008141237   \n",
       "2  PILOT STYLE Touch Pen de marque Speedlink est ...   201115110   938777978   \n",
       "\n",
       "   prdtypecode                cat_name_fr             cat_name_en  \\\n",
       "0           10          Livres d'occasion              Used Books   \n",
       "1         2280        Revues et Magazines  Magazines and Journals   \n",
       "2           50  Accessoires de jeux vidéo  Video Game Accessories   \n",
       "\n",
       "                                            combined  \\\n",
       "0  Olivia: Personalisiertes Notizbuch / 150 Seite...   \n",
       "1  Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...   \n",
       "2  Grand Stylet Ergonomique Bleu Gamepad Nintendo...   \n",
       "\n",
       "                                         comb_tokens  \n",
       "0  ['olivia', 'personalisiertes', 'notizbuch', 's...  \n",
       "1  ['journal', 'arts', 'art', 'marche', 'salon', ...  \n",
       "2  ['stylet', 'ergonomique', 'gamepad', 'nintendo...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROC_DATA_PATH = '/Users/jansta/learn/DS-bootcamp/apr25_bds_rakuten_2/processed_data/'\n",
    "df = pd.read_csv(PROC_DATA_PATH + 'X_train_with_labels_ext.csv') ## this is a df containing changes resulting from the simons-EN file\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdca1906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['notre' 'nicht' 'qualité' 'kein' 'style' 'ancien' 'fossi' 'starai' 'ont'\n",
      " 'nach' 'blanche' 'ebbi' 'son' 'eusse' 'aurai' 'nouvelle' 'fois' 'ob' 'au'\n",
      " 'col' 'five' 'nostre' 'delle' 'nei' 'sarò' 'soyons' 'gt' 'six' 'durch'\n",
      " 'se' 'aveva' 'fossimo' 'noir' 'war' 'werde' 'sta' 'autre' 'sua' 'ed'\n",
      " 'wir' 'bis' 'stette' 'dove' 'avez' 'furono' 'on' 'avremo' 'einen' 'lei'\n",
      " 'faceva' 'hatte' 'facendo' 'avessimo' 'ancienne' 'solches' 'tue' 'class'\n",
      " 'this' 'oui' 'différente' 'contre' 'gegen' 'violette' 'étais' 'warst'\n",
      " 'is' 'agl' 'vostre' 'serai' 'fait' 'einigem' 'mm' 'utilisation' 'en'\n",
      " 'donc' 'von' 'stia' 'fusse' 'vor' 'quanti' 'egrave' 'alles' 'sono'\n",
      " 'verte' 'édition' 'savoir' 'li' 'du' 'dass' 'sto' 'siano' 'derselbe'\n",
      " 'meines' 'allo' 'seront' 'avemmo' 'rsquo' 'br' 'weil' 'avuti']\n"
     ]
    }
   ],
   "source": [
    "### Loading a stopwords numpy array\n",
    "all_stopwords = np.load(PROC_DATA_PATH +'all_stopwords.npy', allow_pickle=True)\n",
    "print(all_stopwords[:100]) ### first 100 stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4986ece8",
   "metadata": {},
   "source": [
    "### TF - IDF Vectorization setup - here it is only for exploration: \n",
    "### in future cases you have to use it in the pipeline\n",
    "### correctly applied to Trained on train data and only applied to test data --> otherise: DATA LEAKAGE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4d31c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84916\n",
      "Unique tokens in 'comb_tokens': 73011\n"
     ]
    }
   ],
   "source": [
    "full = pd.concat([pd.Series(row) for row in X_train['comb_tokens']])\n",
    "print(len(full))\n",
    "vocabulary = set(full)\n",
    "print(f\"Unique tokens in 'comb_tokens': {len(vocabulary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc1c90e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_SIZE = 15000 ## 9999 is likely to small this paramterer has to be explored\n",
    "### one idea would be to tie it to the number of unique words in the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eba376",
   "metadata": {},
   "source": [
    "### specity the tfidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4d1aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "        max_features=VECTOR_SIZE,  # Limit to N number of features\n",
    "        ngram_range=(1,2),  # Unigrams and bigrams\n",
    "        min_df=5, max_df=0.8,  # Ignore very rare and very common terms\n",
    "        lowercase=True,\n",
    "        stop_words=list(all_stopwords),  # Keep product-specific terms\n",
    "        sublinear_tf=True,  # Better for large corpora\n",
    "        norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49924a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### applying it to combined text\n",
    "tfidf_vectors = vectorizer.fit_transform(X_train['combined'])\n",
    "#np.save(DATA_PATH + 'tfidf_vectors.npy', tfidf_vectors.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceafa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23049b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "### applying it to preprocessed tokens --> from comb_tokens column --> from -EN.ipynb notebook\n",
    "tfidf_tok_vectors = vectorizer.fit_transform(X_train['comb_tokens'])\n",
    "#np.save(DATA_PATH + 'tfidf_tok_vectors.npy', tfidf_tok_vectors.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7946d375",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_tok_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c1a120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cc9427",
   "metadata": {},
   "outputs": [],
   "source": [
    "### HERE WE CAN TRY TO USE THE WORD2VEC MODEL\n",
    "### we will use the same preprocessed tokens from the comb_tokens column\n",
    "X_train['comb_tokens'] = X_train['comb_tokens'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0f256d",
   "metadata": {},
   "source": [
    "### this is a demo of a class added to the ../src/functions/text_helpers.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f3e6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecEmbedder:\n",
    "    '''takes a list of tokenized texts and returns their embeddings using Word2Vec'''\n",
    "    def __init__(self, vector_size=300, min_count=2, workers=4):\n",
    "        self.vector_size = vector_size\n",
    "        self.min_count = min_count\n",
    "        self.workers = workers\n",
    "        self.model = None\n",
    "    \n",
    "    def fit(self, token_lists):\n",
    "        print('Training Word2Vec...')\n",
    "        #tokenized = [text.lower().split() for text in token_lists]\n",
    "        self.model = Word2Vec(token_lists, vector_size=self.vector_size, \n",
    "                             min_count=self.min_count, workers=self.workers)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, token_lists):\n",
    "        if self.model is None:\n",
    "            raise ValueError('Model not trained. Call fit() first.')\n",
    "        \n",
    "        def text_to_vector(token_lists):\n",
    "            vectors = [self.model.wv[word] for word in token_lists if word in self.model.wv]\n",
    "            return np.mean(vectors, axis=0) if vectors else np.zeros(self.vector_size)\n",
    "        \n",
    "        embeddings = np.array([text_to_vector(text) for text in token_lists])\n",
    "        print(f'Word2Vec shape: {embeddings.shape}')\n",
    "        return embeddings\n",
    "    \n",
    "    def fit_transform(self, token_lists):\n",
    "        return self.fit(token_lists).transform(token_lists)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c308df",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2VecEmbedder(vector_size=300, min_count=1)\n",
    "w2v.fit(X_train['comb_tokens'][:1000])  # Just first 1000 rows\n",
    "\n",
    "print(f\"Vocabulary size: {len(w2v.model.wv.key_to_index)}\")\n",
    "print(f\"Sample words: {list(w2v.model.wv.key_to_index.keys())[:10]}\")\n",
    "\n",
    "# 2. Check individual embeddings\n",
    "sample_tokens = X_train['comb_tokens'].iloc[0]\n",
    "print(f\"Sample tokens: {sample_tokens}\")\n",
    "embedding = w2v.transform([sample_tokens])\n",
    "print(f\"Embedding shape: {embedding.shape}\")\n",
    "print(f\"Embedding values: {embedding[0][:5]}\")  # First 5 values\n",
    "print(f\"Is all zeros? {np.all(embedding == 0)}\")\n",
    "\n",
    "# 3. Check how many words are being found\n",
    "def debug_transform(tokens):\n",
    "    found_words = [word for word in tokens if word in w2v.model.wv]\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Found in model: {found_words}\")\n",
    "    print(f\"Missing: {[word for word in tokens if word not in w2v.model.wv]}\")\n",
    "\n",
    "debug_transform(sample_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915625f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538e8502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dfd541",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2VecEmbedder(vector_size=300)\n",
    "# Train and transform training data\n",
    "X_train_w2v = w2v.fit_transform(X_train['comb_tokens'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd38246",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "# TOKENIZATION COMPARISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54cdf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PERC = 0.2\n",
    "VOCAB_LIMIT = 40000\n",
    "MAX_SEQUENCE_LENGTH = 75\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee814eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 20.0% of the dataset for training (3396 rows) .\n"
     ]
    }
   ],
   "source": [
    "#df = pd.read_csv(PROC_DATA_PATH + 'X_train_with_labels_ext.csv')\n",
    "\n",
    "print(f'Using {DATASET_PERC*100}% of the dataset for training ({int(df.shape[0]*DATASET_PERC)} rows) .')\n",
    "df = df.head(int(df.shape[0]*DATASET_PERC))\n",
    "## Preprocess the 'comb_tokens' column\n",
    "\n",
    "df['comb_tokens'] = df['comb_tokens'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "all_tokens_filtered = []\n",
    "for token_list in df['comb_tokens']:\n",
    "    all_tokens_filtered.extend(token_list)\n",
    "# Count token frequencies\n",
    "token_counter = Counter(all_tokens_filtered)\n",
    "# Keep only top N most frequent tokens\n",
    "most_common_tokens = dict(token_counter.most_common(VOCAB_LIMIT))\n",
    "vocab_filtered = {word: i+1 for i, word in enumerate(most_common_tokens.keys())}\n",
    "vocab_size_filtered = len(vocab_filtered) + 1\n",
    "\n",
    "def tokens_to_sequences_filtered(token_list):\n",
    "    return [vocab_filtered[token] for token in token_list if token in vocab_filtered]\n",
    "    \n",
    "sequences = [tokens_to_sequences_filtered(tokens) for tokens in df['comb_tokens']]\n",
    "X = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d7a5ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../misc/')\n",
    "from tokenization_qc import analyze_tokenizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c99ab1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jansta/learn/DS-bootcamp/apr25_bds_rakuten_2/notebooks'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1786cbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR KERAS TOKENIZATION:\n",
    "def remove_stopwords(text):\n",
    "    words = str(text).lower().split()\n",
    "    return ' '.join([word for word in words if word not in all_stopwords])\n",
    "\n",
    "df['keras_tokens'] = df['combined'].apply(remove_stopwords)\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = Tokenizer(num_words=VOCAB_LIMIT)\n",
    "tokenizer.fit_on_texts(df['keras_tokens'])\n",
    "sequences = tokenizer.texts_to_sequences(df['keras_tokens'])\n",
    "X = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67c5bb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TOKENIZATION QUALITY ANALYSIS FOR CNN TEXT CLASSIFICATION\n",
      "================================================================================\n",
      "\n",
      "1. FILTERED TOKENIZATION (Nouns/Verbs/Adjs/Advs etc. kept)\n",
      "--------------------------------------------------\n",
      "\n",
      "2. STOPWORD REMOVAL TOKENIZATION\n",
      "--------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "COMPARATIVE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "📊 VOCABULARY STATISTICS\n",
      "------------------------------\n",
      "                                 Metric Filtered (Nouns/Verbs) Stopword Removal\n",
      "                        Vocabulary Size                  20624            29093\n",
      "Effective Vocab (after num_words limit)                  20624            29093\n",
      "                    Avg Sequence Length                   45.3             67.4\n",
      "                    Max Sequence Length                    637              896\n",
      "                    Min Sequence Length                      0                1\n",
      "            Sequences > MAX_SEQ_LEN (%)                  22.9%            34.9%\n",
      "                    Empty Sequences (%)                   0.1%             0.0%\n",
      "\n",
      "📏 SEQUENCE LENGTH ANALYSIS\n",
      "------------------------------\n",
      "Filtered approach - Length percentiles:\n",
      "  25th: 6.0\n",
      "  50th: 23.0\n",
      "  75th: 71.0\n",
      "  95th: 144.0\n",
      "\n",
      "Stopword removal - Length percentiles:\n",
      "  25th: 8.0\n",
      "  50th: 34.0\n",
      "  75th: 101.0\n",
      "  95th: 227.0\n",
      "\n",
      "🎯 INFORMATION DENSITY ANALYSIS\n",
      "------------------------------\n",
      "Average unique token density:\n",
      "  Filtered: 0.844\n",
      "  Stopword: 0.812\n",
      "\n",
      "🔤 VOCABULARY QUALITY INDICATORS\n",
      "------------------------------\n",
      "Top 15 words - Filtered approach:\n",
      "['mate', 'eau', 'tre', 'piscine', 'haute', 'diffe', 'enfant', 'jeu', 'riel', 'inte', 'caracte', 'lumie', 'rieur', 'facile', 'ristiques']\n",
      "\n",
      "Top 15 words - Stopword removal:\n",
      "['-', ':', '/>', '1', '/', '<br', 'haute', 'piscine', '2', 'facile', '3', '*', 'enfants', 'taille:', '/><br']\n",
      "\n",
      "🏷️ CLASS REPRESENTATION ANALYSIS\n",
      "------------------------------\n",
      "Unique vocabulary per class (top 5 classes):\n",
      "  1300: Filtered=3181, Stopword=4470\n",
      "  1560: Filtered=4200, Stopword=5512\n",
      "  2403: Filtered=854, Stopword=1160\n",
      "  2522: Filtered=1772, Stopword=2394\n",
      "  2583: Filtered=3568, Stopword=5344\n",
      "\n",
      "🧠 CNN-SPECIFIC ANALYSIS\n",
      "------------------------------\n",
      "Padding efficiency (lower is better):\n",
      "  Filtered: 54.1% padding tokens\n",
      "  Stopword: 46.3% padding tokens\n",
      "\n",
      "Embedding layer parameters:\n",
      "  Filtered: 20,624 vocab × embedding_dim\n",
      "  Stopword: 29,093 vocab × embedding_dim\n",
      "\n",
      "================================================================================\n",
      "🎯 RECOMMENDATIONS FOR CNN TEXT CLASSIFICATION\n",
      "================================================================================\n",
      "\n",
      "✅ CHOOSE FILTERED (Nouns/Verbs) IF:\n",
      "  • You have limited computational resources\n",
      "  • Your task focuses on content words (entities, actions)\n",
      "  • You want faster training and inference\n",
      "  • Vocabulary size: 20,624 is manageable\n",
      "  • Average sequence length (45.3) fits your needs\n",
      "\n",
      "✅ CHOOSE STOPWORD REMOVAL IF:\n",
      "  • You need to capture more linguistic nuances\n",
      "  • Your classification depends on function words/context\n",
      "  • You have sufficient computational resources\n",
      "  • Longer sequences (67.4 avg) provide better context\n",
      "  • You can handle larger vocabulary (29,093 total)\n",
      "\n",
      "📈 QUALITY SCORES (higher is better):\n",
      "  Filtered approach: 0.62\n",
      "  Stopword removal: 0.60\n",
      "\n",
      "🏆 RECOMMENDATION: Use FILTERED tokenization\n"
     ]
    }
   ],
   "source": [
    "results = analyze_tokenizations(df, MAX_SEQUENCE_LENGTH=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d03779f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gensim-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
