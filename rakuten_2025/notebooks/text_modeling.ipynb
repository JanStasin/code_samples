{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System and utilities\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "# Data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Natural Language Processing (NLP)\n",
    "import re\n",
    "from langdetect import detect\n",
    "import ast\n",
    "\n",
    "# Machine Learning & Modeling\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "## importing helper functions:\n",
    "import sys\n",
    "sys.path.append('../src/functions')\n",
    "#from text_helpers import get_word2vec_features\n",
    "#from text_helpers import Word2VecEmbedder\n",
    "\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Image processing and display\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# Progress bars\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading the data and defining the paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['notre', 'nicht', 'qualité', 'kein', 'style', 'ancien', 'fossi', 'starai', 'ont', 'nach', 'blanche', 'ebbi', 'son', 'eusse', 'aurai', 'nouvelle', 'fois', 'ob', 'au', 'col', 'five', 'nostre', 'delle', 'nei', 'sarò', 'soyons', 'gt', 'six', 'durch', 'se', 'aveva', 'fossimo', 'noir', 'war', 'werde', 'sta', 'autre', 'sua', 'ed', 'wir', 'bis', 'stette', 'dove', 'avez', 'furono', 'on', 'avremo', 'einen', 'lei', 'faceva', 'hatte', 'facendo', 'avessimo', 'ancienne', 'solches', 'tue', 'class', 'this', 'oui', 'différente', 'contre', 'gegen', 'violette', 'étais', 'warst', 'is', 'agl', 'vostre', 'serai', 'fait', 'einigem', 'mm', 'utilisation', 'en', 'donc', 'von', 'stia', 'fusse', 'vor', 'quanti', 'egrave', 'alles', 'sono', 'verte', 'édition', 'savoir', 'li', 'du', 'dass', 'sto', 'siano', 'derselbe', 'meines', 'allo', 'seront', 'avemmo', 'rsquo', 'br', 'weil', 'avuti', 'stemmo', 'by', 'wollen', 'lo', 'können', 'meine', 'avrete', 'toi', 'usage', 'mia', 'es', 'agrave', 'jener', 'stava', 'feci', 'photo', 'daß', 'dessen', 'nbsp', 'haben', 'uno', 'sei', 'eut', 'ins', 'avessi', 'indem', 'fui', 'avais', 'offre', 'hier', 'des', 'sugli', 'einigen', 'anderem', 'dimension', 'aurons', 'demander', 'soyez', 'welcher', 'ihres', 'stareste', 'all', 'eurem', 'nello', 'ensembles', 'dimensions', 'coi', 'starebbe', 'v', 'avevi', 'starebbero', 'è', 'fus', 'gewesen', 'détails', 'dov', 'the', 'ai', 'promotion', 'elles', 'encore', 'ayez', 'bei', 'format', 'voi', 'ten', 'soll', 'stiamo', 'qu', 'la', 'avesse', 'welche', 'petite', 'andern', 'falloir', 'seiner', 'seule', 'disponible', 'hab', 'aus', 'avait', 'einmal', 'été', 'avrò', 'tu', 'fussiez', 'grise', 'aveste', 'di', 'vos', 'avete', 'étantes', 'bin', 'essendo', 'http', \"d'\", 'eacute', 'sulla', 'eût', 'jedes', 'quelli', 'hinter', 'vert', 'eravamo', 'uns', 'be', 'quale', 'del', 'neuf', 'un', 'ensemble', 'quanto', 'couleur', 'j', 'mein', 'a', 'starete', 'fussent', 'hatten', 'avevo', 'dieses', 'anche', 'diese', 'petit', 'dieselben', 'will', 'zwischen', 'quot', 'farei', 'div', 'pour', 'stessimo', 'pièce', 'degl', 'serait', 'depuis', 'trois', 'quel', 'sont', 'plus', 'stavate', 'couleurs', 'avranno', 'mode', 'deiner', 'sareste', 'four', 'de', 'sur', 'den', 'welches', 'wie', 'moins', 'eines', 'sa', 'est', 'diesem', 'eure', 'saremo', 'machen', 'stiate', 'und', 'aurez', 'x', 'starei', 'was', 'vente', 'autres', 'to', 'peut', 'keiner', 'ce', 'élément', 'dies', 'hin', 'so', 'sia', 'noi', 'eux', 'mich', 'eûtes', 'neun', 'wieder', 'stessi', 'vous', 'matière', 'ma', 'kg', 'beige', 'ml', 'facevano', 'foste', 'le', 'jeden', 'saresti', 'dagli', 'marron', 'dich', 'sarei', 'l', 'an', 'sulle', 'cyan', 'faire', 'dall', 'fece', 'était', 'negl', 'mie', 'non', 'client', 'euer', 'dalla', 'ihre', 'drei', 'avions', 'sull', 'argent', 'â', 'ero', 'serais', 'keinem', 'staranno', 'allen', 'dein', 'sue', 'of', 'eine', 'zur', 'stavi', 'avute', 'anderes', 'eussiez', 'être', 'fût', 'sois', 'fussions', 'cui', 'habe', 'that', 'mio', 'die', 'contro', 'ayante', 'facevate', 'avoir', 'kann', 'sind', 'deinem', 'euren', 'chaque', 'come', 'vostri', 'rouge', 'o', 'una', 'fûmes', 'facessero', 'anderen', 'acht', 'me', 'ich', 'fareste', 'stanno', 'sarebbero', 'set', 'peu', 'lot', 'car', 'moi', 'soient', 'étées', 'tes', 'bleu', 'etwas', 'wird', 'farai', 'vier', 'eurer', 'nun', 'faccia', 'garantie', 'welchen', 'vers', 'mir', 'fanno', 'diesen', 'mais', 'serions', 'denn', 'manchem', 'wo', 'dort', 'unseres', 'stando', 'stavo', 'ander', 'tua', 'offert', 'fai', 'étante', 'einige', 'unseren', 'dalle', 'eues', 'avant', 'jedem', 'sous', 'keinen', 'fummo', 'devoir', 'welchem', 'saumon', 'as', 'voir', 'certain', 'éléments', 'deux', 'zwar', 'mit', 'facesti', 'venir', 'allem', 'loro', 'stai', 'tous', 'www', 'agli', 'eus', 'ton', 'dire', 'abbiano', 'seines', 'auras', 'würden', 'ohne', 'facciano', 'grand', 'are', 'êtes', 'alt', 'degli', 'stock', 'dont', 'for', 'et', 'stessero', 'tuo', 'mes', 'paiement', 'perché', 'img', 'serez', 'quello', 'ihn', 'ils', 'haut', 'trouver', 'wenn', 'jaune', 'kitalors', 'sugl', 'https', 'im', 'te', 'suis', 'dieser', 'facessimo', 'jenen', 'nell', 'avesti', 'über', 'starà', 'lui', 'denselben', 'anderer', 'ecirc', 'elle', 'als', 'io', 'ceux', 'occasion', 'two', 'zwei', 'faceste', 'étants', 'ihren', 'ta', \"j'\", 'viel', 'aviez', 'unsere', 'env', 'auriez', 'faresti', 'sept', 'aie', 'étiez', 'zu', 'avrai', 'dazu', 'font', 'y', 'eusses', 'chez', 'sarai', 'ebbero', 'matériau', 'sechs', 'reprise', 'eue', 'wirst', 's', 'das', 'avrei', 'tuoi', 'outils', 'one', 'aucun', 'deinen', 'stesti', 'sich', 'farà', 'après', 'avendo', 'suoi', 'ha', 'offrir', 'dal', 'elf', 'con', 'avessero', 'chi', 'dieselbe', 'p', 'alle', 'sullo', 'ihrer', 'avevamo', 'vom', 'questo', 'collection', 'meinen', 'weiter', 'avevate', 'fu', 'nos', 'questi', 'étions', 'anderr', 'negli', 'solche', 'accessoires', 'derselben', 'e', 'comme', 'farebbe', 'orange', 'at', 'am', 'ayons', 'selbst', 'rose', 'faremo', 'border', 'facessi', 'abbiamo', 'auraient', 'eri', 'fut', 'gli', 'avrebbe', 'nelle', 'pouvoir', 'livraison', 'faremmo', 'aies', 'étant', 'staremo', 'ebbe', 'faranno', 'span', 'fûtes', 'il', 'd', 'avuta', 'doch', 'mettre', 'zwölf', 'commande', 'pièces', 'remise', 'étée', 'assez', 'siate', 'der', 'unser', 'seinen', 'ou', 'par', 'für', 'zum', 'stavano', 'sera', 'musste', 'facevi', 'pourquoi', 'les', 't', 'avreste', 'entre', 'juste', 'seriez', 'einem', 'mancher', 'promo', 'ihm', 'jeder', 'facture', 'g', 'da', 'wollte', 'facesse', 'dans', 'desselben', 'quatre', 'ihnen', 'service', 'cm', 'ein', 'fünf', 'ne', 'aussi', 'saremmo', 'r', 'sollte', 'avremmo', 'muss', 'ses', 'sul', 'avec', 'noire', 'article', 'turquoise', 'aura', 'produit', 'indigo', 'color', 'bordeaux', 'quanta', 'paquet', 'queste', 'soit', 'jene', 'cela', 'sie', 'aurais', 'ayant', 'dir', 'c', 'même', 'questa', 'er', 'saranno', 'nous', 'qui', 'doré', 'quelle', 'demselben', 'ist', 'seras', 'che', 'su', 'it', 'aller', 'pendant', 'ad', 'leur', 'einig', 'solchem', 'oder', 'nostri', 'furent', 'euch', 'erano', 'dai', 'aux', 'avuto', 'keine', 'dello', 'werden', 'comment', 'avrà', 'stetti', 'donner', 'dem', 'könnte', 'and', 'facevo', 'seul', 'ihr', 'eu', 'référence', 'siete', 'zehn', 'matériaux', 'blanc', 'maintenant', 'per', 'ici', 'weg', 'jede', 'nostra', 'hanno', 'manche', 'possible', 'pas', 'deine', 'i', 'huit', 'si', 'stesse', 'unserem', 'avrebbero', 'qualite', 'fosse', 'più', 'al', 'm', 'facevamo', \"qu'\", 'tra', \"n'\", 'einer', 'dorée', 'que', 'marque', 'tutto', 'meiner', 'anders', 'ayantes', 'manchen', 'votre', 'era', 'gris', 'eures', 'three', 'noch', 'dasselbe', 'tout', 'violet', 'nostro', 'man', 'aurait', 'ho', 'eravate', 'vi', 'à', 'vol', 'bleue', 'nouveau', \"l'\", 'abbia', 'auf', 'vouloir', 'aurions', 'sarete', 'solcher', 'avresti', 'waren', 'einiger', 'différent', 'ait', 'seinem', 'fusses', 'sieben', 'stiano', 'ti', 'dagl', 'quella', 'steste', 'bib', 'grande', 'articles', 'alla', 'faccio', 'fosti', 'avons', 'sommes', 'eûmes', 'prix', 'commerce', 'modèle', 'deines', 'jenem', 'ni', 'sein', 'tutti', 'facemmo', 'dell', 'sarà', 'sonst', 'miei', 'eussions', 'dix', 'um', 'vostro', 'ci', 'n', 'seine', 'farò', 'étés', 'pack', 'très', 'mi', 'étaient', 'quante', 'with', 'sondern', 'serons', 'seraient', 'taille', 'src', 'andere', 'würde', 'sui', 'einiges', 'jetzt', 'nichts', 'facciamo', 'sarebbe', 'nella', 'farebbero', 'hai', 'eurent', 'ces', 'from', 'nel', 'sehr', 'quand', 'or', 'échange', 'stettero', 'staremmo', 'avevano', 'hat', 'prendre', 'cuivre', 'während', 'staresti', 'aber', 'f', 'sans', 'stavamo', 'facciate', 'retour', 'also', 'suo', 'starò', 'une', 'accessoire', 'derer', 'della', 'solchen', 'abbiate', 'damit', 'je', 'avaient', 'ihrem', 'manches', 'nur', 'mon', 'unter', 'meinem', 'ayants', 'outil', 'fecero', 'anderm', 'auront', 'siamo', 'etat', 'jenes', 'fossero', 'farete', 'bist', 'auch', 'eussent', 'dei', 'h', 'eight', 'keines', 'cinq', 'type', 'lavande', 'vostra', 'aient', 'kits', 'seven', 'nine', 'in', 'dann', 'dallo']\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = '../data/'\n",
    "PROC_DATA_PATH = '../processed_data/'\n",
    "df = pd.read_csv(PROC_DATA_PATH  + 'X_train_with_labels_ext.csv')\n",
    "all_stopwords = np.load(PROC_DATA_PATH + 'all_stopwords.npy', allow_pickle=True).tolist() ## vecotrizer need a list\n",
    "print(all_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0400af",
   "metadata": {},
   "source": [
    "### Checking the number of unique tokens in the dataset:\n",
    "- this is useful to understand the vocabulary size and complexity of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "full = pd.concat([pd.Series(row) for row in df['comb_tokens']])\n",
    "print(len(full))\n",
    "vocabulary = set(full)\n",
    "print(f\"Unique tokens in 'comb_tokens': {len(vocabulary)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### setting up a few constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed for reproducibility\n",
    "random_state = np.random.seed(66)\n",
    "VECTOR_SIZE = 15000 ### WE have 73011 unique tokens in the text data, so \n",
    "                    #we can use this as a reference point for tfidf vector size 20-30% would be a good size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0b18ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a TF-IDF vectorizer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "        max_features=VECTOR_SIZE,  # Limit to N number of features\n",
    "        ngram_range=(1,2),  # Unigrams and bigrams\n",
    "        min_df=5, max_df=0.8,  # Ignore very rare and very common terms\n",
    "        lowercase=True,\n",
    "        stop_words=all_stopwords,  # Keep product-specific terms\n",
    "        sublinear_tf=True,  # Better for large corpora\n",
    "        norm='l2'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### VARIANT 1: using combined text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['combined']\n",
    "y=df['prdtypecode']\n",
    "\n",
    "print(y.size)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state, stratify=y)\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = vectorizer.fit_transform(X_train)  # fit + transform\n",
    "X_test_tfidf = vectorizer.transform(X_test)        # transform only\n",
    "print(f'TF-IDF matrix shape: {X_train_tfidf.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Models Dictionary: \n",
    "##### - Add more models as needed comment out the ones you don't want to use.\n",
    "##### - We might wanna read up on the parameters and to specify which ones need adjusting\n",
    "##### -  We might wanna consider cross-validation for better model evaluation as a next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        C=1.0,  \n",
    "        class_weight='balanced',  # Handle class imbalance -- DO NOT USE IF SMOTE IS USED -- the function below handles that though\n",
    "        solver='liblinear',  # Good for sparse data\n",
    "        random_state=random_state\n",
    "    ),\n",
    "\n",
    "    'SGD Classifier': SGDClassifier(\n",
    "        loss='log_loss',\n",
    "        alpha=0.0001,  \n",
    "        class_weight='balanced',\n",
    "        max_iter=1000,\n",
    "        random_state=random_state\n",
    "    ),\n",
    "    'Linear SVM': LinearSVC(\n",
    "        C=1.0,\n",
    "        class_weight='balanced',\n",
    "        max_iter=1000,\n",
    "        random_state=random_state\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=20,  \n",
    "        class_weight='balanced',\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1\n",
    "    )}\n",
    "## Uncomment the following lines to include XGBoost in the models dictionary - not sure if it works but worth a try\n",
    "#,\n",
    "    # 'XGBoost': xgb.XGBClassifier(\n",
    "    #     n_estimators=100,\n",
    "    #     max_depth=6,\n",
    "    #     learning_rate=0.1,\n",
    "    #     subsample=0.8,\n",
    "    #     colsample_bytree=0.8,\n",
    "    #     random_state=random_state,\n",
    "    #     n_jobs=-1,\n",
    "    #     eval_metric='mlogloss'  # For multiclass\n",
    "    # )}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### This is the main function to train and evaluate models. \n",
    "- if you provide different/ partial data it should work as long as dimenstions match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval_basic(X_tr, y_tr, X_tst, y_tst, models, balance_classes=False, verbose=True):\n",
    "    '''balance_classes: if True, uses SMOTE to balance classes in training data\n",
    "       verbose: if True, prints additional information about class distribution and model performance\n",
    "       '''\n",
    "    output = {}\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_tr_enc= label_encoder.fit_transform(y_tr)\n",
    "    y_tst_enc = label_encoder.transform(y_tst)\n",
    "\n",
    "    if balance_classes:\n",
    "        print(' SMOTE for class balancing...')\n",
    "        if verbose:\n",
    "            print(f\"Class distribution before SMOTE:\")\n",
    "            unique, counts = np.unique(y_tr_enc, return_counts=True)\n",
    "            for cls, count in zip(unique, counts):\n",
    "                if cls % 5 == 0: \n",
    "                    print(f\"  Class {cls}: {count} samples\")\n",
    "\n",
    "        smote = SMOTE(random_state=random_state, k_neighbors=5,sampling_strategy='auto')  \n",
    "        X_tr, y_tr_enc= smote.fit_resample(X_tr, y_tr_enc)\n",
    "        if verbose:\n",
    "            print(f'Class distribution after SMOTE:')\n",
    "            unique, counts = np.unique(y_tr_enc, return_counts=True)\n",
    "            for cls, count in zip(unique, counts):\n",
    "                if cls % 5 == 0: \n",
    "                    print(f\"  Class {cls}: {count} samples\")\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f'Training {name}')\n",
    "        \n",
    "        # Train model on TRAINING data\n",
    "        if balance_classes: model.set_params(class_weight=None)  \n",
    "\n",
    "        model.fit(X_tr, y_tr_enc)\n",
    "        # Predict on TEST data\n",
    "        y_pred_enc = model.predict(X_tst)\n",
    "        y_pred = label_encoder.inverse_transform(y_pred_enc)\n",
    "        y_tst_orig = label_encoder.inverse_transform(y_tst_enc)\n",
    "        \n",
    "        # Calculate metrics using original labels\n",
    "        f1 = f1_score(y_tst_orig, y_pred, average='macro') \n",
    "        f1_weighted = f1_score(y_tst_orig, y_pred, average='weighted')\n",
    "            \n",
    "        print(f'F1 Score (macro): {f1:.4f}')\n",
    "        print(f'F1 Score (weighted): {f1_weighted:.4f}')\n",
    "\n",
    "        output[name] = {\n",
    "            'model': model,\n",
    "            'f1_score': f1,           # Fixed: variable name\n",
    "            'f1_weighted': f1_weighted,\n",
    "            'predictions': y_pred\n",
    "        }\n",
    "        if verbose:\n",
    "            print(classification_report(y_tst_orig, y_pred))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "#### RUN IT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## LONG RUNTIME WARNING:\n",
    "#### everytime you train and eval the model, it will take a long time to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "output1 = train_and_eval_basic(X_train_tfidf, y_train, X_test_tfidf, y_test, models, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "## use smote for class balancing\n",
    "output1s = train_and_eval_basic(X_train_tfidf, y_train, X_test_tfidf, y_test, models, balance_classes=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### VARIANT 2: using preprocessed tokens instead of raw text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['comb_tokens'] ## !!!!!! the only difference here\n",
    "y=df['prdtypecode']\n",
    "\n",
    "print(y.size)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state, stratify=y)\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "\n",
    "print(\"Vectorizing text...\")\n",
    "X_train_tfidf_tok = vectorizer.fit_transform(X_train)  # fit + transform\n",
    "X_test_tfidf_tok = vectorizer.transform(X_test)        # transform only\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {X_train_tfidf_tok.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "output2 = train_and_eval_basic(X_train_tfidf_tok, y_train, X_test_tfidf_tok, y_test,  models, balance_classes=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "### saving the outputs to local data folder (not shared) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(DATA_PATH + 'output1.npy', output1)\n",
    "np.save(DATA_PATH + 'output1s.npy', output1s)\n",
    "np.save(DATA_PATH + 'output2.npy', output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Variant with word2vec features\n",
    "- we were advised against using word2vec in favour of keras.embedding layer for cnns\n",
    "- i left it here for comparison and educational purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comb_tokens']= df['comb_tokens'].apply(ast.literal_eval) ## !!!\n",
    "\n",
    "X=pd.DataFrame(df['comb_tokens']) ## !!!!!! the only difference here\n",
    "y=df['prdtypecode']\n",
    "\n",
    "print(y.size)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state, stratify=y)\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE VECTOR SIZE IN Word2VecEmbedder is much smaller than the one in TfidfVectorizer, so we need to adjust it\n",
    "W2V_VECTOR_SIZE = 400 # Adjusted vector size for Word2VecEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2VecEmbedder(vector_size=W2V_VECTOR_SIZE)\n",
    "\n",
    "# Train and transform training data\n",
    "X_train_w2v = w2v.fit_transform(X_train['comb_tokens'])\n",
    "\n",
    "# Transform test data with same model\n",
    "X_test_w2v = w2v.transform(X_test['comb_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "output3 = train_and_eval_basic(X_train_w2v, y_train, X_test_w2v, y_test,  models, balance_classes=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59dc7ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48091235",
   "metadata": {},
   "outputs": [],
   "source": [
    "output1 = np.load(DATA_PATH + 'output1.npy', allow_pickle=True).item()\n",
    "output2 =np.load(DATA_PATH + 'output1s.npy', allow_pickle=True).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5345f83e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logistic Regression': {'model': LogisticRegression(max_iter=1000, solver='liblinear'),\n",
       "  'f1_score': 0.7734938224667732,\n",
       "  'f1_weighted': 0.7902879939112404,\n",
       "  'predictions': array([2705, 1302, 1560, ..., 1560, 1302, 1300], shape=(16984,))},\n",
       " 'SGD Classifier': {'model': SGDClassifier(loss='log_loss'),\n",
       "  'f1_score': 0.7342957373976658,\n",
       "  'f1_weighted': 0.7529368805858434,\n",
       "  'predictions': array([1281, 1302, 1560, ..., 1560, 2280, 2280], shape=(16984,))},\n",
       " 'Linear SVM': {'model': LinearSVC(),\n",
       "  'f1_score': 0.783122845011161,\n",
       "  'f1_weighted': 0.8044003932739417,\n",
       "  'predictions': array([1280, 1302, 1560, ..., 1560, 1302, 1300], shape=(16984,))},\n",
       " 'Random Forest': {'model': RandomForestClassifier(max_depth=20, n_jobs=-1),\n",
       "  'f1_score': 0.6164494092352807,\n",
       "  'f1_weighted': 0.6270541941377651,\n",
       "  'predictions': array([1280, 1302, 1560, ..., 1560,   10,   10], shape=(16984,))}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c324cb8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logistic Regression': {'model': LogisticRegression(max_iter=1000, solver='liblinear'),\n",
       "  'f1_score': 0.7711741423618188,\n",
       "  'f1_weighted': 0.7918192486793633,\n",
       "  'predictions': array([2705, 1302, 1560, ..., 1560, 1302, 1300], shape=(16984,))},\n",
       " 'SGD Classifier': {'model': SGDClassifier(loss='log_loss'),\n",
       "  'f1_score': 0.7213511310781017,\n",
       "  'f1_weighted': 0.7440609711593948,\n",
       "  'predictions': array([1281, 1302, 1560, ..., 1560, 2280, 2280], shape=(16984,))},\n",
       " 'Linear SVM': {'model': LinearSVC(),\n",
       "  'f1_score': 0.7696099633981431,\n",
       "  'f1_weighted': 0.7945755067565161,\n",
       "  'predictions': array([1280, 1302, 1560, ..., 1560, 1302, 1300], shape=(16984,))},\n",
       " 'Random Forest': {'model': RandomForestClassifier(max_depth=20, n_jobs=-1),\n",
       "  'f1_score': 0.6152963304539969,\n",
       "  'f1_weighted': 0.6317166737903592,\n",
       "  'predictions': array([1280, 1302, 1560, ..., 1560,   10,   10], shape=(16984,))}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdc6cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-boot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
