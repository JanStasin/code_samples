{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19d25dda-2ed9-45bd-b77f-33bba89b96c1",
   "metadata": {},
   "source": [
    "# Configuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f008615d-4fb7-49e4-82ef-463ed6b7ee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System and utilities\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import io\n",
    "import base64\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "# Data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Natural Language Processing (NLP)\n",
    "import nltk\n",
    "#import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Machine Learning & Modeling\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#from wordcloud import WordCloud\n",
    "\n",
    "# Image processing and display\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# Progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom modules\n",
    "from auto_translate import AutoTranslate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ad8ff1-8b49-4499-887d-be8d2be090b3",
   "metadata": {},
   "source": [
    "# Exploring X_train and Y_train, and merging the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52022dfb-e1a0-4568-8459-f6afca75f024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>prdtypecode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3804725264</td>\n",
       "      <td>1263597046</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>436067568</td>\n",
       "      <td>1008141237</td>\n",
       "      <td>2280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n",
       "      <td>PILOT STYLE Touch Pen de marque Speedlink est ...</td>\n",
       "      <td>201115110</td>\n",
       "      <td>938777978</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Peluche Donald - Europe - Disneyland 2000 (Mar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50418756</td>\n",
       "      <td>457047496</td>\n",
       "      <td>1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>La Guerre Des Tuques</td>\n",
       "      <td>Luc a des id&amp;eacute;es de grandeur. Il veut or...</td>\n",
       "      <td>278535884</td>\n",
       "      <td>1077757786</td>\n",
       "      <td>2705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         designation  \\\n",
       "0  Olivia: Personalisiertes Notizbuch / 150 Seite...   \n",
       "1  Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...   \n",
       "2  Grand Stylet Ergonomique Bleu Gamepad Nintendo...   \n",
       "3  Peluche Donald - Europe - Disneyland 2000 (Mar...   \n",
       "4                               La Guerre Des Tuques   \n",
       "\n",
       "                                         description   productid     imageid  \\\n",
       "0                                                NaN  3804725264  1263597046   \n",
       "1                                                NaN   436067568  1008141237   \n",
       "2  PILOT STYLE Touch Pen de marque Speedlink est ...   201115110   938777978   \n",
       "3                                                NaN    50418756   457047496   \n",
       "4  Luc a des id&eacute;es de grandeur. Il veut or...   278535884  1077757786   \n",
       "\n",
       "   prdtypecode  \n",
       "0           10  \n",
       "1         2280  \n",
       "2           50  \n",
       "3         1280  \n",
       "4         2705  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATA_PATH = '../data/'\n",
    "PROC_DATA_PATH = '../processed_data/'\n",
    "\n",
    "X_train = pd.read_csv(DATA_PATH  + 'X_train_update.csv')\n",
    "X_test = pd.read_csv(DATA_PATH  + 'X_test_update.csv')\n",
    "Y_train = pd.read_csv(DATA_PATH  + 'Y_train_CVw08PX.csv')\n",
    "\n",
    "tables = [('X_train', X_train), ('X_test', X_test), ('Y_train', Y_train)]\n",
    "\n",
    "# for table_name, table in tables:\n",
    "#     print(\"\\n\"+\"=\" * 120)\n",
    "#     print(f\"\\n{table_name}\\n\");\n",
    "#     display(table.head());\n",
    "#     display(table.info());\n",
    "#     print(f\"\\nDuplicates:\\n{table.duplicated().sum()}\")\n",
    "X_train = X_train[~X_train.index.duplicated(keep='first')]\n",
    "Y_train = Y_train[~Y_train.index.duplicated(keep='first')]\n",
    "X_train = X_train.merge(Y_train, how='left', left_index=True, right_index=True, suffixes=('_X_train', '_Y_train'))\n",
    "\n",
    "X_train.drop(['prdtypecode_Y_train'], axis=1, inplace=True, errors='ignore')\n",
    "X_train = X_train.rename(columns={'Unnamed: 0_X_train': 'Unnamed: 0'})\n",
    "\n",
    "cols_to_drop = [col for col in X_train.columns if 'Unnamed' in str(col) or col in ['0', 'index', 'level_0']]\n",
    "X_train.drop(cols_to_drop, axis=1, inplace=True, errors='ignore')\n",
    "display(X_train.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39f508b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'designation', 'description', 'productid', 'imageid'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985b9998",
   "metadata": {},
   "source": [
    "## based on Simons category names and translations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4214a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('''Based on the observation of the word frequencies and wordclouds, we are in capacity to define the product types associated to each code''')\n",
    "\n",
    "prdtypes = {\n",
    "    10: (\"Livres d'occasion\", \"Used Books\"),\n",
    "    40: (\"Jeux vidéo\", \"Video Games\"),\n",
    "    50: (\"Accessoires de jeux vidéo\", \"Video Game Accessories\"),\n",
    "    60: (\"Consoles de jeux vidéo\", \"Video Game Consoles\"),\n",
    "    1140: (\"Figurines Enfant\", \"Children's Figurines\"),\n",
    "    1160: (\"Cartes à Collectionner\", \"Collectible Cards\"),\n",
    "    1180: (\"Figurines Adulte et Jeux de role\", \"Adult Figurines and Role-Playing Games\"),\n",
    "    1280: (\"Jouets\", \"Toys\"),\n",
    "    1281: (\"Jeux de société\", \"Board Games\"),\n",
    "    1300: (\"Jouets télécommandés\", \"Remote Control Toys\"),\n",
    "    1301: (\"Chaussettes bébé\", \"Baby Socks\"),\n",
    "    1302: (\"Pêche Enfant\", \"Children's Fishing\"),\n",
    "    1320: (\"Puériculture\", \"Childcare\"),\n",
    "    1560: (\"Mobilier intérieur\", \"Indoor Furniture\"),\n",
    "    1920: (\"Literie\", \"Bedding\"),\n",
    "    1940: (\"Alimentation\", \"Food\"),\n",
    "    2060: (\"Décoration\", \"Decoration\"),\n",
    "    2220: (\"Animaux\", \"Animals\"),\n",
    "    2280: (\"Revues et Magazines\", \"Magazines and Journals\"),\n",
    "    2403: (\"Lots Magazines, Livres et BDs\", \"Magazine, Book and Comic Bundles\"),\n",
    "    2462: (\"Jeux d'occasion\", \"Used Games\"),\n",
    "    2522: (\"Papeterie\", \"Stationery\"),\n",
    "    2582: (\"Mobilier de jardin\", \"Garden Furniture\"),\n",
    "    2583: (\"Equipement de piscine\", \"Pool Equipment\"),\n",
    "    2585: (\"Entretien\", \"Maintenance\"),\n",
    "    2705: (\"Livres neufs\", \"New Books\"),\n",
    "    2905: (\"Jeux PC\", \"PC Games\")\n",
    "}\n",
    "\n",
    "X_train['cat_name_fr'] = X_train['prdtypecode'].map(lambda x: prdtypes.get(x, (\"NaN\", \"NaN\"))[0])\n",
    "X_train['cat_name_en'] = X_train['prdtypecode'].map(lambda x: prdtypes.get(x, (\"NaN\", \"NaN\"))[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc9a0b5",
   "metadata": {},
   "source": [
    "### PROPOSED code for '''Combininig designation and description, avoiding copies/replicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44946b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_without_duplicates(designation, description):\n",
    "    desig = str(designation).strip() if pd.notna(designation) else ''\n",
    "    desc = str(description).strip() if pd.notna(description) else ''\n",
    "    \n",
    "    if not desig and not desc:\n",
    "        return ''\n",
    "    elif not desig:\n",
    "        return desc\n",
    "    elif not desc:\n",
    "        return desig\n",
    "    elif desig.lower() == desc.lower():  # Exact copies/replicates (case-insensitive)\n",
    "        return desig\n",
    "    elif desig.lower() in desc.lower():  # Designation contained in description\n",
    "        return desc\n",
    "    elif desc.lower() in desig.lower():  # Description contained in designation\n",
    "        return desig\n",
    "    else:\n",
    "        return f\"{desig} {desc}\"  # Both are unique, combine them\n",
    "\n",
    "# Replace your line with:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf4d7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### applying the function to combine 'designation' and 'description' columns\n",
    "X_train['combined'] = X_train.apply(lambda row: combine_without_duplicates(row['designation'], row['description']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac36120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the processed DataFrame to a CSV file\n",
    "X_train.to_csv(PROC_DATA_PATH + 'X_train_with_labels_ext.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be33218",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Or load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c69cd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(PROC_DATA_PATH + 'X_train_with_labels_ext.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788f7b0d-de9a-4ac7-9218-c56bfd48b987",
   "metadata": {},
   "source": [
    "# Data inspection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e619df49",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa662eb-4b51-4eec-9b29-16ba529dffbe",
   "metadata": {},
   "source": [
    "### Unique Product Type Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718a20b3-6192-4fb7-bb4a-4e6f99a589f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prdtypecode_proportions = Y_train['prdtypecode'].value_counts(normalize=True) * 100\n",
    "prdtypecode_count = Y_train['prdtypecode'].nunique()\n",
    "print(f\"\\nThere are {prdtypecode_count} distinct product type codes.\")\n",
    "prdtypecode_proportions = prdtypecode_proportions.sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(14, 2))\n",
    "sns.barplot(x=prdtypecode_proportions.index, y=prdtypecode_proportions.values, order=prdtypecode_proportions.index)\n",
    "plt.title('Proportion of Each Product Type')\n",
    "plt.xlabel('Product Type Code')\n",
    "plt.ylabel('Proportion (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45779bc7-4d9a-439c-b60c-d0b40d475c79",
   "metadata": {},
   "source": [
    "### Products with no product description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312da3d9-389d-4b1b-a065-d4b27a6729cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_null_descriptions_X_train = (X_train[\"description\"].isnull().sum() / X_train[\"description\"].shape[0]) * 100\n",
    "print(f\"The proportion of null descriptions in X_train is {proportion_null_descriptions_X_train:.1f}%\")\n",
    "\n",
    "proportion_null_descriptions_X_test = (X_test[\"description\"].isnull().sum() / X_test[\"description\"].shape[0]) * 100\n",
    "print(f\"The proportion of null descriptions in X_test is {proportion_null_descriptions_X_test:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0bc853-1ff9-4de1-ab89-2369d634dba5",
   "metadata": {},
   "source": [
    "### Creating a custom stopwords list (object) to be saved for re-use later stages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "869f1801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jansta/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download French stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load French stopwords and add custom ones\n",
    "french_stopwords = set(stopwords.words('french'))\n",
    "german_stopwords = set(stopwords.words('german'))\n",
    "italian_stopwords = set(stopwords.words('italian'))\n",
    "spanish_stopwords = set(stopwords.words('spanish'))\n",
    "nl_stopwords = set(stopwords.words('dutch'))\n",
    "custom_stopwords = {\n",
    "    # Balises et éléments HTML/CSS courants\n",
    "    'br', 'nbsp', 'quot', 'http', 'https', 'www', 'img', 'src', 'alt',\n",
    "    'div', 'span', 'style', 'class', 'border', 'font', 'color',\n",
    "\n",
    "    # formatts and CS:\n",
    "    'env', 'bib',\n",
    "\n",
    "    # Descriptions and measures\n",
    "    'voir', 'photo', 'plus', 'cm', 'mm', 'kg', 'ml', 'x', 'l', 'm', 's',\n",
    "    'taille', 'produit', 'prix', 'offre', 'détails', 'neuf', 'garantie',\n",
    "    'qualite', 'qualité', 'sans', 'type', 'chez', 'être', 'tout', 'dont',\n",
    "    'assez', 'dimension', 'dimensions', 'format', 'etat', 'lot', 'paquet', 'pack',\n",
    "    'modèle', 'marque', 'couleur', 'couleurs', 'matière', 'matériau', 'matériaux',\n",
    "    'usage', 'utilisation', 'occasion', 'vente', 'commerce', 'collection', 'édition',\n",
    "    'vol',\n",
    "\n",
    "    # COlors courantes en français\n",
    "    'blanc', 'blanche', 'noir', 'noire', 'rouge', 'bleu', 'bleue', 'vert', 'verte', 'jaune',\n",
    "    'gris', 'grise', 'orange', 'rose', 'marron', 'violet', 'violette', 'turquoise', 'beige',\n",
    "    'argent', 'doré', 'dorée', 'or', 'cuivre', 'cyan', 'indigo', 'lavande', 'saumon', 'bordeaux',\n",
    "\n",
    "    # Common english words\n",
    "    'the', 'of', 'or', 'a', 'agrave', 'and', 'in', 'to', 'for', 'i', 'is', 'it', 'on',\n",
    "    'with', 'this', 'that', 'by', 'from', 'as', 'at', 'an', 'be', 'are',\n",
    "\n",
    "    # Mots isolés ou symboles, ponctuation\n",
    "    'gt', 'p', 'e', 'eacute', 'egrave', 'ecirc', 'g', 'f', 'r', 'v', 'si', 'aussi',\n",
    "    'h', 'â', 'très', 'peut', 'rsquo', 'd\\'', 'l\\'', 'qu\\'', 'j\\'', 'n\\'',\n",
    "\n",
    "    # Adjectifves and adverbs\n",
    "    'grand', 'grande', 'petit', 'petite', 'même', 'autre', 'autres', 'ancien', 'ancienne',\n",
    "    'nouveau', 'nouvelle', 'possible', 'certain', 'seul', 'seule', 'différent', 'différente',\n",
    "\n",
    "    # Verbs and auxiliaries\n",
    "    'avoir', 'être', 'faire', 'pouvoir', 'aller', 'venir', 'devoir', 'prendre', 'mettre',\n",
    "    'dire', 'vouloir', 'savoir', 'falloir', 'voir', 'demander', 'trouver', 'donner',\n",
    "\n",
    "    # Common ecommerce terms \n",
    "    'livraison', 'service', 'client', 'qualité', 'offert', 'offrir', 'occasion', 'garantie',\n",
    "    'retour', 'facture', 'commande', 'référence', 'stock', 'disponible', 'promo', 'promotion',\n",
    "    'échange', 'reprise', 'remise', 'prix', 'paiement', 'mode', 'de', 'à',\n",
    "\n",
    "    # Short connectors courtes et connecteurs\n",
    "    'et', 'ou', 'mais', 'donc', 'or', 'ni', 'car', 'par', 'pour', 'sur', 'sans', 'avec',\n",
    "    'dans', 'entre', 'chez', 'vers', 'contre', 'depuis', 'avant', 'après', 'pendant', \n",
    "\n",
    "    # Letter numbers\n",
    "    'un', 'une', 'deux', 'trois', 'quatre', 'cinq', 'six', 'sept', 'huit', 'neuf', 'dix',\n",
    "    'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten',\n",
    "    'ein', 'zwei', 'drei', 'vier', 'fünf', 'sechs', 'sieben', 'acht', 'neun', 'zehn', 'elf', 'zwölf',\n",
    "\n",
    "    # Noms génériques et trop fréquents en ecommerce\n",
    "    'accessoire', 'accessoires', 'outil', 'outils', 'élément', 'éléments', 'pièce', 'pièces',\n",
    "    'article', 'articles', 'ensemble', 'ensembles', 'set', 'kits', 'kit'\n",
    "\n",
    "    # random miscellaneous words\n",
    "    'alors', 'aucun', 'aussi', 'autre', 'avant', 'avoir', 'car', 'cela', \n",
    "    'ces', 'ceux', 'chaque', 'comme', 'comment', 'dans', 'des', 'deux', \n",
    "    'donc', 'elle', 'elles', 'encore', 'fait', 'fois', 'font', 'haut', \n",
    "    'ici', 'ils', 'juste', 'leur', 'maintenant', 'mais', 'même', 'moins', \n",
    "    'notre', 'nous', 'peut', 'peu', 'pour', 'pourquoi', 'quand', 'quel', \n",
    "    'quelle', 'sans', 'sera', 'sous', 'tout', 'tous', 'très', 'votre', \n",
    "    'vous', 'étaient', 'été', 'être', 'oui'\n",
    "}\n",
    "\n",
    "all_stopwords = french_stopwords.union(custom_stopwords).union(german_stopwords).union(italian_stopwords).union(spanish_stopwords).union(nl_stopwords)  \n",
    "\n",
    "\n",
    "# Define your word grouping dictionary (lemmatization-like rules)\n",
    "word_grouping = {\n",
    "    'livres': 'livre',\n",
    "    'jeux': 'jeu',\n",
    "    'toy':'jeu',\n",
    "    'jouets':'jouet',\n",
    "    'enfants':'enfant',\n",
    "    'agrave': 'a',\n",
    "    'eacute': 'e',\n",
    "    'egrave': 'e',\n",
    "    'ecirc': 'e',\n",
    "    'gravures': 'gravure',\n",
    "    'car':'voiture',\n",
    "    'tools': 'outils'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eab2b7",
   "metadata": {},
   "source": [
    "# Question for us: what about the translations?\n",
    "- Here we should explore more the language distributions and decide when and where do we want to translate.\n",
    "    - all to french?\n",
    "    - ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d51885",
   "metadata": {},
   "source": [
    "## CLEANING TEXT WITH SPACY:\n",
    "- code below loada the spacy models library and creates the function to clean up the text and perform basic tokenization\n",
    "- here is also when translation can be implemented in necessary but it would require some reworking of the code\n",
    "- if (token.pos_ in ['NOUN', 'VERB', 'ADJ', 'ADV', 'NUM', 'PROPN'] --> adds: 'ADJ', 'ADV', 'NUM', 'PROPN' to the previous version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d2f100b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import spacy\n",
    "from langdetect import detect, detect_langs\n",
    "from googletrans import Translator\n",
    "from collections import Counter, defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3fe7bd",
   "metadata": {},
   "source": [
    "### I have added a langauage detection function to identify the language of the text\n",
    "- there is a translation to french added using deep_translator\n",
    "- I have sparated everything by function so it is easier to see what is going on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86f7bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#translator = Translator()\n",
    "# from auto_translate import AutoTranslate\n",
    "# translator = AutoTranslate(\"fr\")\n",
    "from deep_translator import GoogleTranslator\n",
    "translator = GoogleTranslator(source='auto', target='fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9a91bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dutch model not found. Install with: python -m spacy download nl_core_news_sm\n"
     ]
    }
   ],
   "source": [
    "def load_spacy_models():\n",
    "    models = {}\n",
    "    try:\n",
    "        models['fr'] = spacy.load(\"fr_core_news_sm\")\n",
    "    except OSError:\n",
    "        print(\"French model not found. Install with: python -m spacy download fr_core_news_sm\")\n",
    "        models['fr'] = None\n",
    "    \n",
    "    try:\n",
    "        models['en'] = spacy.load(\"en_core_web_sm\") \n",
    "    except OSError:\n",
    "        print(\"English model not found. Install with: python -m spacy download en_core_web_sm\")\n",
    "        models['en'] = None\n",
    "        \n",
    "    try:\n",
    "        models['de'] = spacy.load(\"de_core_news_sm\")\n",
    "    except OSError:\n",
    "        print(\"German model not found. Install with: python -m spacy download de_core_news_sm\")\n",
    "        models['de'] = None\n",
    "    \n",
    "    try:\n",
    "        models['es'] = spacy.load(\"es_core_news_sm\")\n",
    "    except OSError:\n",
    "        print(\"Spanish model not found. Install with: python -m spacy download es_core_news_sm\")\n",
    "        models['es'] = None\n",
    "    \n",
    "    try:\n",
    "        models['it'] = spacy.load(\"it_core_news_sm\")\n",
    "    except OSError:\n",
    "        print(\"Italian model not found. Install with: python -m spacy download it_core_news_sm\")\n",
    "        models['it'] = None\n",
    "\n",
    "    try:\n",
    "        models['nl'] = spacy.load(\"n;_core_news_sm\")\n",
    "    except OSError:\n",
    "        print(\"dutch model not found. Install with: python -m spacy download nl_core_news_sm\")\n",
    "        models['nl'] = None\n",
    "    \n",
    "    return models\n",
    "\n",
    "SPACY_MODELS = load_spacy_models()\n",
    "\n",
    "# Define stopwords for different languages\n",
    "STOPWORDS_BY_LANG = {\n",
    "    'fr': {'le', 'de', 'et', 'à', 'un', 'il', 'être', 'et', 'en', 'avoir', 'que', 'pour', 'dans', 'ce', 'son', 'une', 'sur', 'avec', 'ne', 'se', 'pas', 'mais', 'plus', 'dire', 'son', 'tout'},\n",
    "    'en': {'the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'it', 'for', 'not', 'on', 'with', 'he', 'as', 'you', 'do', 'at', 'this', 'but', 'his', 'by', 'from'},\n",
    "    'de': {'der', 'die', 'und', 'in', 'den', 'von', 'zu', 'das', 'mit', 'sich', 'des', 'auf', 'für', 'ist', 'im', 'dem', 'nicht', 'ein', 'eine', 'als', 'auch', 'es', 'an', 'werden'},\n",
    "    'es': {'el', 'la', 'de', 'que', 'y', 'a', 'en', 'un', 'ser', 'se', 'no', 'te', 'lo', 'le', 'da', 'su', 'por', 'son', 'con', 'para', 'al', 'como', 'las', 'pero', 'sus'},\n",
    "    'it': {'il', 'di', 'che', 'e', 'la', 'per', 'un', 'in', 'è', 'non', 'da', 'con', 'le', 'si', 'del', 'una', 'ma', 'più', 'su', 'anche', 'se', 'come', 'sono', 'lui'}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d479eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_text_language(text, min_length=20):\n",
    "    \"\"\"Detect language of text with confidence scores\"\"\"\n",
    "    if len(text) < min_length:\n",
    "        return 'fr', 0.5  # Default to French for short texts\n",
    "    \n",
    "    try:\n",
    "        lang_probs = detect_langs(text)\n",
    "        if lang_probs:\n",
    "            primary_lang = lang_probs[0]\n",
    "            return primary_lang.lang, primary_lang.prob\n",
    "        else:\n",
    "            return 'fr', 0.5\n",
    "    except Exception as e:\n",
    "        return 'fr', 0.5\n",
    "\n",
    "def get_language_distribution():\n",
    "    \"\"\"Get the current language distribution from processed texts\"\"\"\n",
    "    if not LANGUAGE_STATS:\n",
    "        return {}\n",
    "    \n",
    "    total_chars = sum(LANGUAGE_STATS.values())\n",
    "    distribution = {}\n",
    "    \n",
    "    for lang, char_count in LANGUAGE_STATS.items():\n",
    "        distribution[lang] = {\n",
    "            'percentage': (char_count / total_chars) * 100 if total_chars > 0 else 0,\n",
    "            'char_count': char_count\n",
    "        }\n",
    "    \n",
    "    return distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afd07e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bab402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_to_french(text, source_lang):\n",
    "    \"\"\"Translate text to French using deep-translator\"\"\"\n",
    "    if source_lang == 'fr' or not text.strip():\n",
    "        return text\n",
    "    \n",
    "    try:\n",
    "        # Create translator for specific language pair\n",
    "        if source_lang in ['en', 'es', 'de', 'it', 'pt', 'nl', 'ru', 'zh', 'ja', 'ko']:\n",
    "            translator = GoogleTranslator(source=source_lang, target='fr')\n",
    "        else:\n",
    "            # Fallback to auto-detect\n",
    "            translator = GoogleTranslator(source='auto', target='fr')\n",
    "        \n",
    "        # Split into chunks if text is too long (deep-translator handles this better)\n",
    "        max_length = 4500  # Conservative limit\n",
    "        if len(text) > max_length:\n",
    "            words = text.split()\n",
    "            chunks = []\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "            \n",
    "            for word in words:\n",
    "                if current_length + len(word) + 1 > max_length:\n",
    "                    if current_chunk:\n",
    "                        chunks.append(' '.join(current_chunk))\n",
    "                        current_chunk = [word]\n",
    "                        current_length = len(word)\n",
    "                    else:\n",
    "                        # Single word too long, add it anyway\n",
    "                        chunks.append(word)\n",
    "                        current_length = 0\n",
    "                else:\n",
    "                    current_chunk.append(word)\n",
    "                    current_length += len(word) + 1\n",
    "            \n",
    "            if current_chunk:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "            \n",
    "            translated_chunks = []\n",
    "            for chunk in chunks:\n",
    "                if chunk.strip():\n",
    "                    try:\n",
    "                        result = translator.translate(chunk)\n",
    "                        translated_chunks.append(result)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Translation failed for chunk: {e}\")\n",
    "                        translated_chunks.append(chunk)\n",
    "            \n",
    "            return ' '.join(translated_chunks)\n",
    "        else:\n",
    "            result = translator.translate(text)\n",
    "            return result\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Translation failed for {source_lang} -> fr: {e}\")\n",
    "        return text  # Return original text if translation fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98cd870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fallback_method(text, stopwords):\n",
    "    \"\"\"Fallback method without POS tagging\"\"\"\n",
    "    tokens = text.split()\n",
    "    cleaned_tokens = []\n",
    "    \n",
    "    for w in tokens:\n",
    "        if w not in stopwords and len(w) > 2:\n",
    "            if w in word_grouping:\n",
    "                cleaned_tokens.append(word_grouping[w])\n",
    "            else:\n",
    "                cleaned_tokens.append(w)\n",
    "    \n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48b5ea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Main text cleaning function - compatible with pandas apply()\n",
    "    Returns cleaned tokens as a list\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    # 1. Detect language\n",
    "    detected_lang, confidence = detect_text_language(text)\n",
    "    \n",
    "    # Store language stats\n",
    "    LANGUAGE_STATS[detected_lang] += len(text)\n",
    "    \n",
    "    # 2. Basic text cleaning\n",
    "    text_normalized = unicodedata.normalize('NFD', text).lower()\n",
    "    \n",
    "    # Handle contractions for different languages\n",
    "    if detected_lang == 'fr':\n",
    "        contractions = {\n",
    "            r\"\\bl'\": \"le \", r\"\\bd'\": \"de \", r\"\\bc'\": \"ce \", r\"\\bj'\": \"je \",\n",
    "            r\"\\bn'\": \"ne \", r\"\\bm'\": \"me \", r\"\\bt'\": \"te \", r\"\\bs'\": \"se \",\n",
    "            r\"\\bqu'\": \"que \"\n",
    "        }\n",
    "        for pattern, replacement in contractions.items():\n",
    "            text_normalized = re.sub(pattern, replacement, text_normalized)\n",
    "    elif detected_lang == 'en':\n",
    "        contractions = {\n",
    "            r\"\\bcan't\": \"cannot\", r\"\\bwon't\": \"will not\", r\"\\bn't\": \" not\",\n",
    "            r\"\\b're\": \" are\", r\"\\b've\": \" have\", r\"\\b'll\": \" will\",\n",
    "            r\"\\b'd\": \" would\"\n",
    "        }\n",
    "        for pattern, replacement in contractions.items():\n",
    "            text_normalized = re.sub(pattern, replacement, text_normalized)\n",
    "    \n",
    "    # Replace ligatures and remove HTML\n",
    "    text_normalized = text_normalized.replace('œ', 'oe').replace('æ', 'ae')\n",
    "    text_normalized = re.sub(r'<[^>]+>', ' ', text_normalized)\n",
    "    \n",
    "    # Language-specific character normalization\n",
    "    if detected_lang in ['fr', 'es', 'it']:\n",
    "        text_normalized = re.sub(r'[^a-zàâäçéèêëîïôöûùüÿñáíóúü\\s]', ' ', text_normalized)\n",
    "    elif detected_lang == 'de':\n",
    "        text_normalized = re.sub(r'[^a-zäöüß\\s]', ' ', text_normalized)\n",
    "    else:\n",
    "        text_normalized = re.sub(r'[^a-z\\s]', ' ', text_normalized)\n",
    "    \n",
    "    text_normalized = re.sub(r'\\s+', ' ', text_normalized).strip()\n",
    "    \n",
    "    # 3. spaCy POS tagging and filtering\n",
    "    cleaned_tokens = []\n",
    "    \n",
    "    # Choose appropriate spaCy model\n",
    "    nlp = None\n",
    "    if detected_lang in SPACY_MODELS and SPACY_MODELS[detected_lang]:\n",
    "        nlp = SPACY_MODELS[detected_lang]\n",
    "    elif SPACY_MODELS['en']:\n",
    "        nlp = SPACY_MODELS['en']\n",
    "    elif SPACY_MODELS['fr']:\n",
    "        nlp = SPACY_MODELS['fr']\n",
    "    \n",
    "    # Get stopwords for detected language\n",
    "    stopwords = STOPWORDS_BY_LANG.get(detected_lang, STOPWORDS_BY_LANG.get('en', set()))\n",
    "    \n",
    "    if nlp:\n",
    "        try:\n",
    "            doc = nlp(text_normalized)\n",
    "            \n",
    "            for token in doc:\n",
    "                if (token.pos_ in ['NOUN', 'VERB', 'ADJ', 'ADV', 'NUM', 'PROPN'] and \n",
    "                    len(token.text) > 2 and \n",
    "                    not token.is_stop and\n",
    "                    token.text.lower() not in stopwords and\n",
    "                    token.text.lower() not in all_stopwords):\n",
    "                    \n",
    "                    word_text = token.text.lower()\n",
    "                    \n",
    "                    # Apply word grouping\n",
    "                    if word_text in word_grouping:\n",
    "                        cleaned_tokens.append(word_grouping[word_text])\n",
    "                    else:\n",
    "                        cleaned_tokens.append(word_text)\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"spaCy processing failed: {e}, using fallback\")\n",
    "            cleaned_tokens = fallback_method(text_normalized, stopwords)\n",
    "    else:\n",
    "        cleaned_tokens = fallback_method(text_normalized, stopwords)\n",
    "    \n",
    "    # 4. Translation to French if not already French\n",
    "    if detected_lang != 'fr' and cleaned_tokens:\n",
    "        # Join tokens back to text for translation\n",
    "        text_to_translate = ' '.join(cleaned_tokens)\n",
    "        translated_text = translate_to_french(text_to_translate, detected_lang)\n",
    "        \n",
    "        # Re-tokenize translated text if translation occurred\n",
    "        if translated_text != text_to_translate:\n",
    "            # Simple re-tokenization for translated text\n",
    "            translated_tokens = []\n",
    "            translated_normalized = unicodedata.normalize('NFD', translated_text).lower()\n",
    "            translated_normalized = re.sub(r'[^a-zàâäçéèêëîïôöûùüÿñ\\s]', ' ', translated_normalized)\n",
    "            translated_normalized = re.sub(r'\\s+', ' ', translated_normalized).strip()\n",
    "            \n",
    "            french_stopwords = STOPWORDS_BY_LANG.get('fr', set())\n",
    "            for word in translated_normalized.split():\n",
    "                if (word not in french_stopwords and \n",
    "                    word not in all_stopwords and \n",
    "                    len(word) > 2):\n",
    "                    if word in word_grouping:\n",
    "                        translated_tokens.append(word_grouping[word])\n",
    "                    else:\n",
    "                        translated_tokens.append(word)\n",
    "            \n",
    "            cleaned_tokens = translated_tokens\n",
    "    \n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cae85090",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LANGUAGE_STATS = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eae1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear previous stats\n",
    "LANGUAGE_STATS.clear()\n",
    "\n",
    "# Process exactly as you specified\n",
    "X_train['comb_tokens_fr'] = X_train['combined'].dropna().astype(str).apply(clean_text)\n",
    "\n",
    "# Get language distribution after processing\n",
    "distribution = get_language_distribution()\n",
    "\n",
    "print(\"Results:\")\n",
    "for i, row in X_train.iterrows():\n",
    "    print(f\"Text {i+1}: {row['combined'][:50]}...\")\n",
    "    print(f\"Tokens: {row['comb_tokens_fr']}\")\n",
    "    print()\n",
    "\n",
    "print(\"Language Distribution:\")\n",
    "for lang, stats in distribution.items():\n",
    "    print(f\"  {lang}: {stats['percentage']:.1f}% ({stats['char_count']} characters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63690193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "Text 1: Olivia: Personalisiertes Notizbuch / 150 Seiten / ...\n",
      "Tokens: ['olivia', 'personnalise', 'sites', 'din', 'roses', 'design']\n",
      "\n",
      "Text 10001: Protege Cahier Pvc 18/100 24x32 Vert...\n",
      "Tokens: ['protege', 'cahier']\n",
      "\n",
      "Text 20001: Repose tête pour Spa - Blanc Moment de détente abs...\n",
      "Tokens: ['repose', 'spa', 'moment', 'absolu', 'spa', 'repose', 'facile', 'installer', 'fiche', 'technique', 'appuie', 'spa', 'conc', 'spas', 'intex', 'installer', 'repositionne', 'envies', 'forme', 'englobe', 'cou', 'caracteristiques', 'techniques', 'poids']\n",
      "\n",
      "Text 30001: Tabouret Stoolsonline - Sweet Bar Stool Adjustable...\n",
      "Tokens: ['tabouret', 'toolsonline', 'tabouret', 'bar', 'sucre', 'glable', 'tabouret', 'bar', 'parfait', 'tabouret', 'relaxant', 'tasse', 'cafe', 'journal', 'pre', 'simple', 'ale', 'ance', 'serein', 'gance', 'pivotante']\n",
      "\n",
      "Text 40001: Papier Peint Autocollant Aspect Satine Moutarde (1...\n",
      "Tokens: ['papier', 'peint', 'autocollant', 'aspect', 'satine', 'moutarde', 'adhesif', 'ideal', 'recouvrir', 'meubles', 'ikea', 'electromenager', 'surfaces', 'lisses', 'murs', 'adhesifs', 'utilisables', 'interieur', 'exterieur', 'adaptes', 'professionnel', 'plotters', 'decoupe', 'resiste', 'eau', 'bien', 'poser', 'adhesif', 'nettoyez', 'degraissez', 'bien', 'surface', 'appliquez', 'eau', 'savonneuse', 'aide', 'pulve', 'surface', 'propre', 'posez', 'delicatement', 'adhesif', 'metre', 'coin', 'surface', 'recouvrir', 'raclette', 'feutrine', 'chassez', 'lentement', 'bulles', 'air', 'etalez', 'lentement', 'adhesif', 'surface', 'repetant', 'etape', 'laissez', 'secher', 'heure', 'astuce', 'fanny', 'bulles', 'air', 'temps', 'sechage', 'utilisez', 'aiguille', 'fine', 'percer', 'delicatement', 'chasser', 'air', 'raclette']\n",
      "\n",
      "Text 50001: Pompe piscine pcclair 2 cv mono Pompe de filtratio...\n",
      "Tokens: ['pompe', 'piscine', 'pcclair', 'mono', 'pompe', 'filtration', 'piscine', 'pcclair', 'mono', 'bit', 'fabrique', 'piscine', 'center', 'charte', 'clair', 'faible', 'niveau', 'sonore', 'rendement', 'pre', 'filtre', 'large', 'capacite']\n",
      "\n",
      "Text 60001: Européenne Home Decor 45 * 45cm Coussin En Lin De ...\n",
      "Tokens: ['europe', 'enne', 'home', 'decor', 'coussin', 'lin', 'coton', 'couverture', 'jeter', 'taie', 'europe', 'enne', 'home', 'decor', 'coussin', 'lin', 'coton', 'couverture', 'jeter', 'spe', 'cifications', 'taie', 'mate', 'riel', 'forme', 'lin', 'place', 'taie', 'oreiller', 'parfait', 'applications', 'sident', 'sofe', 'voiture', 'seat', 'coration', 'cover', 'taie', 'oreiller', 'taie', 'oreiller', 'durable', 'respectueux', 'environnement', 'confortable', 'plai', 'permettre', 'viation', 'mesure', 'plai', 'comprendre', 'contenu', 'taie', 'oreiller', 'inte', 'rieur', 'oreiller', 'inclus']\n",
      "\n",
      "Text 70001: La France Et Ses Trésors Collection Larousse Régio...\n",
      "Tokens: ['france', 'tre', 'sors', 'larousse', 'gion', 'limousin', 'dition', 'pages']\n",
      "\n",
      "Text 80001: Écharpe De Portage Manduca Sling - Rouge...\n",
      "Tokens: ['charpe', 'portage', 'manduca', 'sling']\n",
      "\n",
      "Language Distribution:\n"
     ]
    }
   ],
   "source": [
    "# Get language distribution after processing\n",
    "distribution = get_language_distribution()\n",
    "\n",
    "print(\"Results:\")\n",
    "for i, row in X_train.iterrows():\n",
    "    if i % 10000 == 0:\n",
    "        print(f\"Text {i+1}: {row['combined'][:50]}...\")\n",
    "        print(f\"Tokens: {row['comb_tokens_fr']}\")\n",
    "        print()\n",
    "\n",
    "print(\"Language Distribution:\")\n",
    "for lang, stats in distribution.items():\n",
    "    print(f\"  {lang}: {stats['percentage']:.1f}% ({stats['char_count']} characters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5f2b11d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        ['olivia', 'personnalise', 'sites', 'din', 'ro...\n",
       "1        ['journal', 'arts', 'art', 'marche', 'salon', ...\n",
       "2        ['stylet', 'ergonomique', 'gamepad', 'nintendo...\n",
       "3        ['peluche', 'donald', 'europe', 'disneyland', ...\n",
       "4        ['guerre', 'tuques', 'luc', 'grandeur', 'veut'...\n",
       "                               ...                        \n",
       "84911                        ['sims', 'import', 'anglais']\n",
       "84912    ['piscine', 'acier', 'nevada', 'pierre', 'desc...\n",
       "84913    ['journal', 'officiel', 'republique', 'francai...\n",
       "84914    ['table', 'basse', 'bois', 'cupe', 'ration', '...\n",
       "84915              ['gomme', 'gommes', 'pinguin', 'glace']\n",
       "Name: comb_tokens_fr, Length: 84916, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['comb_tokens_fr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2df1f14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f6eb5e0",
   "metadata": {},
   "source": [
    "## THIS IS THE OLD FUNCTION WITHOUT TRANSLATION TO FRENCH AND FEWER LANGAGES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247d8a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models once\n",
    "SPACY_MODELS = load_spacy_models()\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    manual_translations = {\n",
    "        'monde': 'world', 'enfant': 'child', 'enfants': 'child', 'voiture': 'car', 'maison': 'house',\n",
    "        'revue': 'magazine', 'livre': 'book', 'vie': 'life', 'doudou': 'stuffed animal',\t\n",
    "        'gravure': 'engraving', 'gravures': 'engraving', 'haute': 'high', 'jardin': 'garden', 'facile': 'easy',\n",
    "        'chaise': 'chair', 'bois': 'wood', 'diffe': 'different', 'ditions': 'editions',\n",
    "        'dition': 'edition', 'eau': 'water', 'titres': 'titles', 'collier': 'necklace', 'sac': 'bag',\n",
    "        'chien': 'dog', 'masque': 'mask', 'commune': 'common', 'guerre': 'war', 'cumple': 'couple'\n",
    "    }\n",
    "    \n",
    "    # 1. Detect language\n",
    "    detected_lang = 'fr'  # Default to French\n",
    "    try:\n",
    "        detected_lang = detect(text)\n",
    "        #print(f\"Detected language: {detected_lang}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Language detection failed: {e}, using French as default {text[:50]}\")\n",
    "    \n",
    "    # 2. Basic text cleaning\n",
    "    text = unicodedata.normalize('NFD', text).lower()\n",
    "    \n",
    "    # Handle French contractions\n",
    "    contractions = {\n",
    "        r\"\\bl'\": \"le \", r\"\\bd'\": \"de \", r\"\\bc'\": \"ce \", r\"\\bj'\": \"je \",\n",
    "        r\"\\bn'\": \"ne \", r\"\\bm'\": \"me \", r\"\\bt'\": \"te \", r\"\\bs'\": \"se \",\n",
    "        r\"\\bqu'\": \"que \"\n",
    "    }\n",
    "    for pattern, replacement in contractions.items():\n",
    "        text = re.sub(pattern, replacement, text)\n",
    "    \n",
    "    # Replace ligatures and remove HTML\n",
    "    text = text.replace('œ', 'oe').replace('æ', 'ae')\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    \n",
    "    # Keep French characters and normalize whitespace\n",
    "    text = re.sub(r'[^a-zàâäçéèêëîïôöûùüÿñ\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # 3. spaCy POS tagging and filtering\n",
    "    cleaned_tokens = []\n",
    "    \n",
    "    # Choose appropriate spaCy model\n",
    "    nlp = None\n",
    "    if detected_lang in SPACY_MODELS and SPACY_MODELS[detected_lang]:\n",
    "        nlp = SPACY_MODELS[detected_lang]\n",
    "    elif SPACY_MODELS['fr']:  # Fallback to French\n",
    "        nlp = SPACY_MODELS['fr']\n",
    "    elif SPACY_MODELS['en']:  # Fallback to English\n",
    "        nlp = SPACY_MODELS['en']\n",
    "    \n",
    "    if nlp:\n",
    "        try:\n",
    "            # Process text with spaCy\n",
    "            doc = nlp(text)\n",
    "            \n",
    "            for token in doc:\n",
    "                # Filter: only nouns and verbs, skip short words and stopwords\n",
    "                if (token.pos_ in ['NOUN', 'VERB', 'ADJ', 'ADV', 'NUM', 'PROPN'] and \n",
    "                    len(token.text) > 2 and \n",
    "                    not token.is_stop and\n",
    "                    token.text.lower() not in all_stopwords):\n",
    "                    \n",
    "                    word_text = token.text.lower()\n",
    "                    \n",
    "                    # # Apply manual translations first\n",
    "                    # if word_text in manual_translations:\n",
    "                    #     cleaned_tokens.append(manual_translations[word_text])\n",
    "                    # Apply word grouping\n",
    "                    if word_text in word_grouping:\n",
    "                        cleaned_tokens.append(word_grouping[word_text])\n",
    "                    else:\n",
    "                        cleaned_tokens.append(word_text)\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"spaCy processing failed: {e}, using fallback\")\n",
    "            cleaned_tokens = fallback_method(text, manual_translations)\n",
    "    else:\n",
    "        print(\"No spaCy models available, using fallback method\")\n",
    "        cleaned_tokens = fallback_method(text, manual_translations)\n",
    "    \n",
    "    return cleaned_tokens\n",
    "\n",
    "def fallback_method(text, manual_translations):\n",
    "    \"\"\"Fallback method without POS tagging\"\"\"\n",
    "    tokens = text.split()\n",
    "    cleaned_tokens = []\n",
    "    \n",
    "    for w in tokens:\n",
    "        if w not in all_stopwords and len(w) > 2:\n",
    "            # if w in manual_translations:\n",
    "            #     cleaned_tokens.append(manual_translations[w])\n",
    "            if w in word_grouping:\n",
    "                cleaned_tokens.append(word_grouping[w])\n",
    "            else:\n",
    "                cleaned_tokens.append(w)\n",
    "    \n",
    "    return cleaned_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083ab4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the clean_text function to the 'combined' column\n",
    "#X_train['comb_tokens'] = X_train['combined'].dropna().astype(str).apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87112f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed DataFrame with the new 'comb_tokens' column\n",
    "#X_train.to_csv(PROC_DATA_PATH + 'X_train_with_labels_ext.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b5325d",
   "metadata": {},
   "source": [
    "### CREATING THE MOST COMMON WORDS PER PRODUCT TYPE CODE\n",
    " - code based on on Simons work on the same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c37255-b2cc-4d95-a69f-55e4aa9c5b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get product types\n",
    "product_types = X_train['prdtypecode'].unique()\n",
    "\n",
    "top = 100\n",
    "get_english = False # Set to True to translate words to English\n",
    "save_dictionary = True  # Set to True to save the top words in a dictionary\n",
    "top_data = {}\n",
    "top_data['product_type'] = []\n",
    "top_data['designation'] = []\n",
    "top_data['description'] = []\n",
    "top_data['combined'] = []\n",
    "\n",
    "for idx, prd_type in enumerate(product_types[:]):\n",
    "    type_data = X_train[X_train['prdtypecode'] == prd_type]\n",
    "    type_data = type_data.copy()\n",
    "    \n",
    "    designation_tokens = type_data['designation'].dropna().astype(str).apply(clean_text).sum()\n",
    "    description_tokens = type_data['description'].dropna().astype(str).apply(clean_text).sum()\n",
    "    combined_tokens = type_data['combined'].dropna().astype(str).apply(clean_text).sum()\n",
    "\n",
    "    # designation_text = ' '.join(designation_tokens)\n",
    "    # description_text = ' '.join(description_tokens)\n",
    "    # full_text = ' '.join(combined_tokens)\n",
    "\n",
    "    # Frequency counts separately\n",
    "    designation_counts = Counter(designation_tokens)\n",
    "    description_counts = Counter(description_tokens)\n",
    "    combined_counts = Counter(combined_tokens)\n",
    "\n",
    "    top_designation = designation_counts.most_common(top)\n",
    "    top_description = description_counts.most_common(top)\n",
    "    top_combined = combined_counts.most_common(top)\n",
    "\n",
    "    top_data['product_type'].append(prd_type)\n",
    "\n",
    "    if get_english:\n",
    "        top_data['designation'].append([(translate.translate(word), freq) for word, freq in top_designation])\n",
    "        top_data['description'].append([(translate.translate(word), freq) for word, freq in top_description])\n",
    "        top_data['combined'].append([(translate.translate(word), freq) for word, freq in top_combined])\n",
    "\n",
    "\n",
    "    else:\n",
    "\n",
    "        top_data['designation'].append(top_designation)\n",
    "        top_data['description'].append(top_description)\n",
    "        top_data['combined'].append(top_combined)\n",
    "        \n",
    "    df_designation = pd.DataFrame(top_designation, columns=['Word (Designation)', 'Frequency'])\n",
    "    df_description = pd.DataFrame(top_description, columns=['Word (Description)', 'Frequency'])\n",
    "    df_combined = pd.DataFrame(top_combined, columns=['Word (combined)', 'Frequency'])\n",
    "    if idx % 5 == 0:\n",
    "        print(f\"\\nProcessing product type {prd_type} ({idx + 1}/{len(product_types)})\")\n",
    "        # Display top 20 words side-by-side\n",
    "        display(HTML(f\"<h3>Top 20 Words for Product Type {prd_type}</h3>\"))\n",
    "        display(pd.concat([df_designation, df_description, df_combined], axis=1))\n",
    "\n",
    "        # Print product samples\n",
    "        print(f\"\\n📦 Sample Products for Product Type {prd_type}\")\n",
    "        print(\"=\" * 60)\n",
    "        display(type_data[['designation', 'description', 'combined']].sample(min(5, len(type_data)), random_state=42))\n",
    "\n",
    "        # Other stats\n",
    "        print(f\"\\n📊 Stats for Product Type {prd_type}\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Products: {len(type_data)}\")\n",
    "        print(f\"Unique designations: {type_data['designation'].nunique()}\")\n",
    "        print(f\"Unique descriptions: {type_data['description'].nunique()}\")\n",
    "        print(f\"Unique combined: {type_data['combined'].nunique()}\")\n",
    "        print(f\"Avg designation length: {type_data['designation'].str.len().mean():.0f} characters\")\n",
    "        print(f\"Avg description length: {type_data['description'].str.len().mean():.0f} characters\")\n",
    "        print(f\"Avg combined length: {type_data['combined'].str.len().mean():.0f} characters\")\n",
    "        print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "if save_dictionary:\n",
    "    # Save the top words in a dictionary\n",
    "    print(f\"Saving top {top} words in a dictionary...\")\n",
    "    np.save(DATA_PATH + f'top_{top}_fr_data.npy', top_data, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088b6e76",
   "metadata": {},
   "source": [
    "### Creating wordclouds based on the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee23b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True to translate words to English\n",
    "start_time = time.time()\n",
    "for prd_type in product_types[:5]:\n",
    "    type_data = X_train[X_train['prdtypecode'] == prd_type]\n",
    "    type_data = type_data.copy()\n",
    "    type_data['combined'] = type_data.apply(lambda row: combine_without_duplicates(row['designation'], row['description']), axis=1)\n",
    "    \n",
    "    designation_tokens = type_data['designation'].dropna().astype(str).apply(clean_text).sum()\n",
    "    description_tokens = type_data['description'].dropna().astype(str).apply(clean_text).sum()\n",
    "    combined_tokens = type_data['combined'].dropna().astype(str).apply(clean_text).sum()\n",
    "    #combined_tokens2 = [translate.translate(word) for word in combined_tokens if word] if get_english else combined_tokens\n",
    "\n",
    "    designation_text = ' '.join(designation_tokens)\n",
    "    description_text = ' '.join(description_tokens)\n",
    "    combined_text = ' '.join(combined_tokens)\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 8))\n",
    "\n",
    "    if designation_text:\n",
    "        wc1 = WordCloud(width=800, height=400, background_color='white',\n",
    "                        colormap='Reds', max_words=100).generate(designation_text)\n",
    "        ax1.imshow(wc1, interpolation='bilinear')\n",
    "        ax1.set_title(f'Designations - Product Type {prd_type}', fontsize=16, fontweight='bold')\n",
    "        ax1.axis('off')\n",
    "    else:\n",
    "        ax1.text(0.5, 0.5, 'No designation data', ha='center', va='center', transform=ax1.transAxes)\n",
    "        ax1.axis('off')\n",
    "\n",
    "    if description_text:\n",
    "        wc2 = WordCloud(width=800, height=400, background_color='white',\n",
    "                        colormap='Blues', max_words=100).generate(description_text)\n",
    "        ax2.imshow(wc2, interpolation='bilinear')\n",
    "        ax2.set_title(f'Descriptions - Product Type {prd_type}', fontsize=16, fontweight='bold')\n",
    "        ax2.axis('off')\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'No description data', ha='center', va='center', transform=ax2.transAxes)\n",
    "        ax2.axis('off')\n",
    "\n",
    "    if combined_text:\n",
    "        wc3 = WordCloud(width=800, height=400, background_color='white',\n",
    "                        colormap='Blues', max_words=100).generate(combined_text)\n",
    "        ax3.imshow(wc3, interpolation='bilinear')\n",
    "        ax3.set_title(f'Combined - Product Type {prd_type}', fontsize=16, fontweight='bold')\n",
    "        ax3.axis('off')\n",
    "    else:\n",
    "        ax3.text(0.5, 0.5, 'No description data', ha='center', va='center', transform=ax3.transAxes)\n",
    "        ax3.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Time taken to generate word clouds: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "#np.save(DATA_PATH + 'combined_descriptionsENG', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a365df65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#np.save(DATA_PATH + f'top_{top}_fr_data.npy', top_data, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7652e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
