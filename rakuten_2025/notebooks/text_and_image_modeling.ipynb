{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8885330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System and utilities\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "# Data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Natural Language Processing (NLP)\n",
    "import re\n",
    "#from langdetect import detect\n",
    "\n",
    "# Machine Learning & Modeling\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src/functions')\n",
    "#from text_helpers import get_word2vec_features\n",
    "#from text_helpers import Word2VecEmbedder\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Image processing and display\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# Progress bars\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "560ac955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12d9b728",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/'\n",
    "PROC_DATA_PATH = '../processed_data/'\n",
    "df = pd.read_csv(PROC_DATA_PATH  + 'X_train_with_labels.csv')\n",
    "#df_img_flat = pd.read_csv(DATA_PATH + 'flattened_images_32.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0defb1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>prdtypecode</th>\n",
       "      <th>cat_name_fr</th>\n",
       "      <th>cat_name_en</th>\n",
       "      <th>combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3804725264</td>\n",
       "      <td>1263597046</td>\n",
       "      <td>10</td>\n",
       "      <td>Livres d'occasion</td>\n",
       "      <td>Used Books</td>\n",
       "      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         designation description   productid  \\\n",
       "0  Olivia: Personalisiertes Notizbuch / 150 Seite...         NaN  3804725264   \n",
       "\n",
       "      imageid  prdtypecode        cat_name_fr cat_name_en  \\\n",
       "0  1263597046           10  Livres d'occasion  Used Books   \n",
       "\n",
       "                                            combined  \n",
       "0  Olivia: Personalisiertes Notizbuch / 150 Seite...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ae6f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['notre', 'nicht', 'qualité', 'kein', 'style', 'ancien', 'fossi', 'starai', 'ont', 'nach', 'blanche', 'ebbi', 'son', 'eusse', 'aurai', 'nouvelle', 'fois', 'ob', 'au', 'col', 'five', 'nostre', 'delle', 'nei', 'sarò', 'soyons', 'gt', 'six', 'durch', 'se', 'aveva', 'fossimo', 'noir', 'war', 'werde', 'sta', 'autre', 'sua', 'ed', 'wir', 'bis', 'stette', 'dove', 'avez', 'furono', 'on', 'avremo', 'einen', 'lei', 'faceva', 'hatte', 'facendo', 'avessimo', 'ancienne', 'solches', 'tue', 'class', 'this', 'oui', 'différente', 'contre', 'gegen', 'violette', 'étais', 'warst', 'is', 'agl', 'vostre', 'serai', 'fait', 'einigem', 'mm', 'utilisation', 'en', 'donc', 'von', 'stia', 'fusse', 'vor', 'quanti', 'egrave', 'alles', 'sono', 'verte', 'édition', 'savoir', 'li', 'du', 'dass', 'sto', 'siano', 'derselbe', 'meines', 'allo', 'seront', 'avemmo', 'rsquo', 'br', 'weil', 'avuti', 'stemmo', 'by', 'wollen', 'lo', 'können', 'meine', 'avrete', 'toi', 'usage', 'mia', 'es', 'agrave', 'jener', 'stava', 'feci', 'photo', 'daß', 'dessen', 'nbsp', 'haben', 'uno', 'sei', 'eut', 'ins', 'avessi', 'indem', 'fui', 'avais', 'offre', 'hier', 'des', 'sugli', 'einigen', 'anderem', 'dimension', 'aurons', 'demander', 'soyez', 'welcher', 'ihres', 'stareste', 'all', 'eurem', 'nello', 'ensembles', 'dimensions', 'coi', 'starebbe', 'v', 'avevi', 'starebbero', 'è', 'fus', 'gewesen', 'détails', 'dov', 'the', 'ai', 'promotion', 'elles', 'encore', 'ayez', 'bei', 'format', 'voi', 'ten', 'soll', 'stiamo', 'qu', 'la', 'avesse', 'welche', 'petite', 'andern', 'falloir', 'seiner', 'seule', 'disponible', 'hab', 'aus', 'avait', 'einmal', 'été', 'avrò', 'tu', 'fussiez', 'grise', 'aveste', 'di', 'vos', 'avete', 'étantes', 'bin', 'essendo', 'http', \"d'\", 'eacute', 'sulla', 'eût', 'jedes', 'quelli', 'hinter', 'vert', 'eravamo', 'uns', 'be', 'quale', 'del', 'neuf', 'un', 'ensemble', 'quanto', 'couleur', 'j', 'mein', 'a', 'starete', 'fussent', 'hatten', 'avevo', 'dieses', 'anche', 'diese', 'petit', 'dieselben', 'will', 'zwischen', 'quot', 'farei', 'div', 'pour', 'stessimo', 'pièce', 'degl', 'serait', 'depuis', 'trois', 'quel', 'sont', 'plus', 'stavate', 'couleurs', 'avranno', 'mode', 'deiner', 'sareste', 'four', 'de', 'sur', 'den', 'welches', 'wie', 'moins', 'eines', 'sa', 'est', 'diesem', 'eure', 'saremo', 'machen', 'stiate', 'und', 'aurez', 'x', 'starei', 'was', 'vente', 'autres', 'to', 'peut', 'keiner', 'ce', 'élément', 'dies', 'hin', 'so', 'sia', 'noi', 'eux', 'mich', 'eûtes', 'neun', 'wieder', 'stessi', 'vous', 'matière', 'ma', 'kg', 'beige', 'ml', 'facevano', 'foste', 'le', 'jeden', 'saresti', 'dagli', 'marron', 'dich', 'sarei', 'l', 'an', 'sulle', 'cyan', 'faire', 'dall', 'fece', 'était', 'negl', 'mie', 'non', 'client', 'euer', 'dalla', 'ihre', 'drei', 'avions', 'sull', 'argent', 'â', 'ero', 'serais', 'keinem', 'staranno', 'allen', 'dein', 'sue', 'of', 'eine', 'zur', 'stavi', 'avute', 'anderes', 'eussiez', 'être', 'fût', 'sois', 'fussions', 'cui', 'habe', 'that', 'mio', 'die', 'contro', 'ayante', 'facevate', 'avoir', 'kann', 'sind', 'deinem', 'euren', 'chaque', 'come', 'vostri', 'rouge', 'o', 'una', 'fûmes', 'facessero', 'anderen', 'acht', 'me', 'ich', 'fareste', 'stanno', 'sarebbero', 'set', 'peu', 'lot', 'car', 'moi', 'soient', 'étées', 'tes', 'bleu', 'etwas', 'wird', 'farai', 'vier', 'eurer', 'nun', 'faccia', 'garantie', 'welchen', 'vers', 'mir', 'fanno', 'diesen', 'mais', 'serions', 'denn', 'manchem', 'wo', 'dort', 'unseres', 'stando', 'stavo', 'ander', 'tua', 'offert', 'fai', 'étante', 'einige', 'unseren', 'dalle', 'eues', 'avant', 'jedem', 'sous', 'keinen', 'fummo', 'devoir', 'welchem', 'saumon', 'as', 'voir', 'certain', 'éléments', 'deux', 'zwar', 'mit', 'facesti', 'venir', 'allem', 'loro', 'stai', 'tous', 'www', 'agli', 'eus', 'ton', 'dire', 'abbiano', 'seines', 'auras', 'würden', 'ohne', 'facciano', 'grand', 'are', 'êtes', 'alt', 'degli', 'stock', 'dont', 'for', 'et', 'stessero', 'tuo', 'mes', 'paiement', 'perché', 'img', 'serez', 'quello', 'ihn', 'ils', 'haut', 'trouver', 'wenn', 'jaune', 'kitalors', 'sugl', 'https', 'im', 'te', 'suis', 'dieser', 'facessimo', 'jenen', 'nell', 'avesti', 'über', 'starà', 'lui', 'denselben', 'anderer', 'ecirc', 'elle', 'als', 'io', 'ceux', 'occasion', 'two', 'zwei', 'faceste', 'étants', 'ihren', 'ta', \"j'\", 'viel', 'aviez', 'unsere', 'env', 'auriez', 'faresti', 'sept', 'aie', 'étiez', 'zu', 'avrai', 'dazu', 'font', 'y', 'eusses', 'chez', 'sarai', 'ebbero', 'matériau', 'sechs', 'reprise', 'eue', 'wirst', 's', 'das', 'avrei', 'tuoi', 'outils', 'one', 'aucun', 'deinen', 'stesti', 'sich', 'farà', 'après', 'avendo', 'suoi', 'ha', 'offrir', 'dal', 'elf', 'con', 'avessero', 'chi', 'dieselbe', 'p', 'alle', 'sullo', 'ihrer', 'avevamo', 'vom', 'questo', 'collection', 'meinen', 'weiter', 'avevate', 'fu', 'nos', 'questi', 'étions', 'anderr', 'negli', 'solche', 'accessoires', 'derselben', 'e', 'comme', 'farebbe', 'orange', 'at', 'am', 'ayons', 'selbst', 'rose', 'faremo', 'border', 'facessi', 'abbiamo', 'auraient', 'eri', 'fut', 'gli', 'avrebbe', 'nelle', 'pouvoir', 'livraison', 'faremmo', 'aies', 'étant', 'staremo', 'ebbe', 'faranno', 'span', 'fûtes', 'il', 'd', 'avuta', 'doch', 'mettre', 'zwölf', 'commande', 'pièces', 'remise', 'étée', 'assez', 'siate', 'der', 'unser', 'seinen', 'ou', 'par', 'für', 'zum', 'stavano', 'sera', 'musste', 'facevi', 'pourquoi', 'les', 't', 'avreste', 'entre', 'juste', 'seriez', 'einem', 'mancher', 'promo', 'ihm', 'jeder', 'facture', 'g', 'da', 'wollte', 'facesse', 'dans', 'desselben', 'quatre', 'ihnen', 'service', 'cm', 'ein', 'fünf', 'ne', 'aussi', 'saremmo', 'r', 'sollte', 'avremmo', 'muss', 'ses', 'sul', 'avec', 'noire', 'article', 'turquoise', 'aura', 'produit', 'indigo', 'color', 'bordeaux', 'quanta', 'paquet', 'queste', 'soit', 'jene', 'cela', 'sie', 'aurais', 'ayant', 'dir', 'c', 'même', 'questa', 'er', 'saranno', 'nous', 'qui', 'doré', 'quelle', 'demselben', 'ist', 'seras', 'che', 'su', 'it', 'aller', 'pendant', 'ad', 'leur', 'einig', 'solchem', 'oder', 'nostri', 'furent', 'euch', 'erano', 'dai', 'aux', 'avuto', 'keine', 'dello', 'werden', 'comment', 'avrà', 'stetti', 'donner', 'dem', 'könnte', 'and', 'facevo', 'seul', 'ihr', 'eu', 'référence', 'siete', 'zehn', 'matériaux', 'blanc', 'maintenant', 'per', 'ici', 'weg', 'jede', 'nostra', 'hanno', 'manche', 'possible', 'pas', 'deine', 'i', 'huit', 'si', 'stesse', 'unserem', 'avrebbero', 'qualite', 'fosse', 'più', 'al', 'm', 'facevamo', \"qu'\", 'tra', \"n'\", 'einer', 'dorée', 'que', 'marque', 'tutto', 'meiner', 'anders', 'ayantes', 'manchen', 'votre', 'era', 'gris', 'eures', 'three', 'noch', 'dasselbe', 'tout', 'violet', 'nostro', 'man', 'aurait', 'ho', 'eravate', 'vi', 'à', 'vol', 'bleue', 'nouveau', \"l'\", 'abbia', 'auf', 'vouloir', 'aurions', 'sarete', 'solcher', 'avresti', 'waren', 'einiger', 'différent', 'ait', 'seinem', 'fusses', 'sieben', 'stiano', 'ti', 'dagl', 'quella', 'steste', 'bib', 'grande', 'articles', 'alla', 'faccio', 'fosti', 'avons', 'sommes', 'eûmes', 'prix', 'commerce', 'modèle', 'deines', 'jenem', 'ni', 'sein', 'tutti', 'facemmo', 'dell', 'sarà', 'sonst', 'miei', 'eussions', 'dix', 'um', 'vostro', 'ci', 'n', 'seine', 'farò', 'étés', 'pack', 'très', 'mi', 'étaient', 'quante', 'with', 'sondern', 'serons', 'seraient', 'taille', 'src', 'andere', 'würde', 'sui', 'einiges', 'jetzt', 'nichts', 'facciamo', 'sarebbe', 'nella', 'farebbero', 'hai', 'eurent', 'ces', 'from', 'nel', 'sehr', 'quand', 'or', 'échange', 'stettero', 'staremmo', 'avevano', 'hat', 'prendre', 'cuivre', 'während', 'staresti', 'aber', 'f', 'sans', 'stavamo', 'facciate', 'retour', 'also', 'suo', 'starò', 'une', 'accessoire', 'derer', 'della', 'solchen', 'abbiate', 'damit', 'je', 'avaient', 'ihrem', 'manches', 'nur', 'mon', 'unter', 'meinem', 'ayants', 'outil', 'fecero', 'anderm', 'auront', 'siamo', 'etat', 'jenes', 'fossero', 'farete', 'bist', 'auch', 'eussent', 'dei', 'h', 'eight', 'keines', 'cinq', 'type', 'lavande', 'vostra', 'aient', 'kits', 'seven', 'nine', 'in', 'dann', 'dallo']\n"
     ]
    }
   ],
   "source": [
    "all_stopwords = np.load(PROC_DATA_PATH + 'all_stopwords.npy', allow_pickle=True).tolist()\n",
    "print(all_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8038313",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = np.random.random(66)\n",
    "VECTOR_SIZE = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20232f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "        max_features=VECTOR_SIZE,  # Limit to N number of features\n",
    "        ngram_range=(1,2),  # Unigrams and bigrams\n",
    "        min_df=5, max_df=0.8,  # Ignore very rare and very common terms\n",
    "        lowercase=True,\n",
    "        stop_words=all_stopwords,  # Keep product-specific terms\n",
    "        sublinear_tf=True,  # Better for large corpora\n",
    "        norm='l2'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696cc724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text data: 84916 rows\n",
      "Merged data with images: 84916 rows\n"
     ]
    }
   ],
   "source": [
    "img_size = 128 #image size\n",
    "images_df = pd.read_csv(PROC_DATA_PATH + f'flattened_images_{img_size}.csv')  # Replace with your image file path\n",
    "#images_df = pd.read_csv(DATA_PATH + 'flattened_images_{128}.csv')\n",
    "# Merge on both productid and imageid\n",
    "df_merged = df.merge(images_df[['productid', 'imageid', 'pixels']], \n",
    "                     on=['productid', 'imageid'], \n",
    "                     how='inner')  # Only keep rows with both text and image\n",
    "\n",
    "print(f\"Original text data: {len(df)} rows\")\n",
    "print(f\"Merged data with images: {len(df_merged)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4983f39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## optional non-shared save\n",
    "df_merged.to_csv(DATA_PATH + f'X_train_labeled_with_flatimgs_{img_size}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5382f025",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text = df_merged['combined']\n",
    "X_images = df_merged['pixels']  # Already vectorized images\n",
    "y = df_merged['prdtypecode'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aeb1a0",
   "metadata": {},
   "source": [
    "### Runing train.test split before vectorization!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff018d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_train, X_text_test, X_img_train, X_img_test, y_train, y_test = train_test_split(X_text, X_images, y, test_size=0.2, random_state=rs, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9416c3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_train_tfidf = vectorizer.fit_transform(X_text_train)\n",
    "X_text_test_tfidf = vectorizer.transform(X_text_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177ce522",
   "metadata": {},
   "source": [
    "### this step may require some attention and arrays checking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e820c81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X_img_train_array = np.array(X_img_train.tolist())\n",
    "# X_img_test_array = np.array(X_img_test.tolist())\n",
    "def clean_image_vector(img):\n",
    "    if isinstance(img, str):\n",
    "        # Remove brackets and split, then filter out '...'\n",
    "        values = [x for x in img.strip('[]').split() if x != '...']\n",
    "        return np.array(values, dtype=np.float32)\n",
    "    else:\n",
    "        return np.array(img, dtype=np.float32)\n",
    "\n",
    "X_img_train_array = np.array([clean_image_vector(img) for img in X_img_train])\n",
    "X_img_test_array = np.array([clean_image_vector(img) for img in X_img_test])\n",
    "\n",
    "print(X_img_train_array.shape)\n",
    "                      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cc3e9e",
   "metadata": {},
   "source": [
    "### combining text and image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "22f2fd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine text and image features\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "X_train_joint = hstack([X_text_train_tfidf, csr_matrix(X_img_train_array)])\n",
    "X_test_joint = hstack([X_text_test_tfidf, csr_matrix(X_img_test_array)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "16eff9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined feature matrix shape: (67932, 10005)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Combined feature matrix shape: {X_train_joint.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a5332b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        C=1.0,  \n",
    "        class_weight='balanced',  # Handle class imbalance\n",
    "        solver='liblinear',  # Good for sparse data\n",
    "        random_state=rs\n",
    "    ),\n",
    "    # 'SGD Classifier': SGDClassifier(\n",
    "    #     loss='log_loss',\n",
    "    #     alpha=0.0001,  \n",
    "    #     class_weight='balanced',\n",
    "    #     max_iter=1000,\n",
    "    #     random_state=rs\n",
    "    # ),\n",
    "    'Linear SVM': LinearSVC(\n",
    "        C=1.0,\n",
    "        class_weight='balanced',\n",
    "        max_iter=1000,\n",
    "        random_state=rs\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=20,  \n",
    "        class_weight='balanced',\n",
    "        random_state=rs,\n",
    "        n_jobs=-1\n",
    "    )}\n",
    "\n",
    "#,\n",
    "    # 'XGBoost': xgb.XGBClassifier(\n",
    "    #     n_estimators=100,\n",
    "    #     max_depth=6,\n",
    "    #     learning_rate=0.1,\n",
    "    #     subsample=0.8,\n",
    "    #     colsample_bytree=0.8,\n",
    "    #     random_state=rs,\n",
    "    #     n_jobs=-1,\n",
    "    #     eval_metric='mlogloss'  # For multiclass\n",
    "    # )}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7644e8",
   "metadata": {},
   "source": [
    "### This is the main function to train and evaluate models. \n",
    "##### -  if you provide different/ partial data it should work as long as dimenstions match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046fb7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval_basic(X_tr, y_tr, X_tst, y_tst, models, balance_classes=False, verbose=True):\n",
    "    '''balance_classes: if True, uses SMOTE to balance classes in training data\n",
    "       verbose: if True, prints additional information about class distribution and model performance\n",
    "       '''\n",
    "    output = {}\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_tr_enc= label_encoder.fit_transform(y_tr)\n",
    "    y_tst_enc = label_encoder.transform(y_tst)\n",
    "\n",
    "    if balance_classes:\n",
    "        print(' SMOTE for class balancing...')\n",
    "        if verbose:\n",
    "            print(f\"Class distribution before SMOTE:\")\n",
    "            unique, counts = np.unique(y_tr_enc, return_counts=True)\n",
    "            for cls, count in zip(unique, counts):\n",
    "                if cls % 5 == 0: \n",
    "                    print(f\"  Class {cls}: {count} samples\")\n",
    "\n",
    "        smote = SMOTE(random_state=rs, k_neighbors=5,sampling_strategy='auto')  \n",
    "        X_tr, y_tr_enc= smote.fit_resample(X_tr, y_tr_enc)\n",
    "        if verbose:\n",
    "            print(f'Class distribution after SMOTE:')\n",
    "            unique, counts = np.unique(y_tr_enc, return_counts=True)\n",
    "            for cls, count in zip(unique, counts):\n",
    "                if cls % 5 == 0: \n",
    "                    print(f\"  Class {cls}: {count} samples\")\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f'Training {name}')\n",
    "        \n",
    "        # Train model on TRAINING data\n",
    "        if balance_classes: model.set_params(class_weight=None)  \n",
    "\n",
    "        model.fit(X_tr, y_tr_enc)\n",
    "        # Predict on TEST data\n",
    "        y_pred_enc = model.predict(X_tst)\n",
    "        y_pred = label_encoder.inverse_transform(y_pred_enc)\n",
    "        y_tst_orig = label_encoder.inverse_transform(y_tst_enc)\n",
    "        \n",
    "        # Calculate metrics using original labels\n",
    "        f1 = f1_score(y_tst_orig, y_pred, average='macro') \n",
    "        f1_weighted = f1_score(y_tst_orig, y_pred, average='weighted')\n",
    "            \n",
    "        print(f'F1 Score (macro): {f1:.4f}')\n",
    "        print(f'F1 Score (weighted): {f1_weighted:.4f}')\n",
    "\n",
    "        output[name] = {\n",
    "            'model': model,\n",
    "            'f1_score': f1,           # Fixed: variable name\n",
    "            'f1_weighted': f1_weighted,\n",
    "            'predictions': y_pred\n",
    "        }\n",
    "        if verbose:\n",
    "            print(classification_report(y_tst_orig, y_pred))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16a26c6",
   "metadata": {},
   "source": [
    "## RUN IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ddd9f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LONG RUNTIME WARNING:\n",
    "#### everytime you train and eval the model, it will take a long time to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ee1fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SMOTE for class balancing...\n",
      "Class distribution before SMOTE:\n",
      "  Class 0: 2493 samples\n",
      "  Class 5: 3162 samples\n",
      "  Class 10: 646 samples\n",
      "  Class 15: 642 samples\n",
      "  Class 20: 1137 samples\n",
      "  Class 25: 2209 samples\n",
      "Class distribution after SMOTE:\n",
      "  Class 0: 8167 samples\n",
      "  Class 5: 8167 samples\n",
      "  Class 10: 8167 samples\n",
      "  Class 15: 8167 samples\n",
      "  Class 20: 8167 samples\n",
      "  Class 25: 8167 samples\n",
      "Training Logistic Regression\n"
     ]
    }
   ],
   "source": [
    "output = train_and_eval_basic(X_train_joint, y_train, X_test_joint, y_test, \n",
    "                                models, \n",
    "                                balance_classes=True, \n",
    "                                verbose=True\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b4159a",
   "metadata": {},
   "source": [
    "## RUN IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a16d9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression\n",
      "F1 Score (macro): 0.7636\n",
      "F1 Score (weighted): 0.7828\n",
      "Training SGD Classifier\n",
      "F1 Score (macro): 0.7279\n",
      "F1 Score (weighted): 0.7471\n",
      "Training Linear SVM\n",
      "F1 Score (macro): 0.7716\n",
      "F1 Score (weighted): 0.7961\n",
      "Training Random Forest\n",
      "F1 Score (macro): 0.6144\n",
      "F1 Score (weighted): 0.6267\n"
     ]
    }
   ],
   "source": [
    "output1 = train_and_eval_basic(X_train_tfidf, y_train, X_test_tfidf, y_test, models, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8253fc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SMOTE for class balancing...\n",
      "Class distribution before SMOTE:\n",
      "  Class 0: 2493 samples\n",
      "  Class 5: 3162 samples\n",
      "  Class 10: 646 samples\n",
      "  Class 15: 642 samples\n",
      "  Class 20: 1137 samples\n",
      "  Class 25: 2209 samples\n",
      "Class distribution after SMOTE:\n",
      "  Class 0: 8167 samples\n",
      "  Class 5: 8167 samples\n",
      "  Class 10: 8167 samples\n",
      "  Class 15: 8167 samples\n",
      "  Class 20: 8167 samples\n",
      "  Class 25: 8167 samples\n",
      "Training Logistic Regression\n",
      "F1 Score (macro): 0.7622\n",
      "F1 Score (weighted): 0.7845\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          10       0.42      0.54      0.47       623\n",
      "          40       0.65      0.63      0.64       502\n",
      "          50       0.74      0.82      0.78       336\n",
      "          60       0.88      0.87      0.88       166\n",
      "        1140       0.72      0.73      0.73       534\n",
      "        1160       0.90      0.87      0.89       791\n",
      "        1180       0.44      0.68      0.53       153\n",
      "        1280       0.74      0.51      0.61       974\n",
      "        1281       0.53      0.54      0.54       414\n",
      "        1300       0.87      0.93      0.90      1009\n",
      "        1301       0.86      0.93      0.89       161\n",
      "        1302       0.71      0.81      0.76       498\n",
      "        1320       0.78      0.75      0.76       648\n",
      "        1560       0.84      0.80      0.82      1015\n",
      "        1920       0.87      0.92      0.90       861\n",
      "        1940       0.64      0.87      0.74       161\n",
      "        2060       0.80      0.76      0.78       999\n",
      "        2220       0.67      0.92      0.77       165\n",
      "        2280       0.75      0.75      0.75       952\n",
      "        2403       0.81      0.60      0.69       955\n",
      "        2462       0.68      0.72      0.70       284\n",
      "        2522       0.91      0.88      0.90       998\n",
      "        2582       0.73      0.76      0.74       518\n",
      "        2583       0.98      0.95      0.97      2042\n",
      "        2585       0.74      0.85      0.79       499\n",
      "        2705       0.65      0.70      0.67       552\n",
      "        2905       1.00      0.99      1.00       174\n",
      "\n",
      "    accuracy                           0.78     16984\n",
      "   macro avg       0.75      0.78      0.76     16984\n",
      "weighted avg       0.79      0.78      0.78     16984\n",
      "\n",
      "Training SGD Classifier\n",
      "F1 Score (macro): 0.7159\n",
      "F1 Score (weighted): 0.7384\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          10       0.34      0.57      0.42       623\n",
      "          40       0.63      0.45      0.53       502\n",
      "          50       0.66      0.75      0.70       336\n",
      "          60       0.91      0.82      0.86       166\n",
      "        1140       0.69      0.72      0.70       534\n",
      "        1160       0.88      0.83      0.85       791\n",
      "        1180       0.44      0.50      0.47       153\n",
      "        1280       0.70      0.44      0.54       974\n",
      "        1281       0.49      0.48      0.49       414\n",
      "        1300       0.81      0.90      0.85      1009\n",
      "        1301       0.86      0.91      0.88       161\n",
      "        1302       0.71      0.70      0.71       498\n",
      "        1320       0.76      0.63      0.69       648\n",
      "        1560       0.76      0.75      0.76      1015\n",
      "        1920       0.86      0.89      0.88       861\n",
      "        1940       0.57      0.84      0.68       161\n",
      "        2060       0.71      0.71      0.71       999\n",
      "        2220       0.67      0.89      0.77       165\n",
      "        2280       0.69      0.74      0.72       952\n",
      "        2403       0.84      0.56      0.68       955\n",
      "        2462       0.66      0.71      0.68       284\n",
      "        2522       0.90      0.82      0.86       998\n",
      "        2582       0.60      0.73      0.66       518\n",
      "        2583       0.97      0.92      0.94      2042\n",
      "        2585       0.68      0.74      0.71       499\n",
      "        2705       0.55      0.73      0.63       552\n",
      "        2905       0.95      0.99      0.97       174\n",
      "\n",
      "    accuracy                           0.74     16984\n",
      "   macro avg       0.72      0.73      0.72     16984\n",
      "weighted avg       0.75      0.74      0.74     16984\n",
      "\n",
      "Training Linear SVM\n",
      "F1 Score (macro): 0.7601\n",
      "F1 Score (weighted): 0.7871\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          10       0.42      0.51      0.46       623\n",
      "          40       0.64      0.64      0.64       502\n",
      "          50       0.72      0.78      0.75       336\n",
      "          60       0.88      0.84      0.86       166\n",
      "        1140       0.71      0.73      0.72       534\n",
      "        1160       0.89      0.87      0.88       791\n",
      "        1180       0.45      0.65      0.53       153\n",
      "        1280       0.73      0.58      0.65       974\n",
      "        1281       0.53      0.56      0.54       414\n",
      "        1300       0.93      0.93      0.93      1009\n",
      "        1301       0.80      0.92      0.86       161\n",
      "        1302       0.74      0.81      0.77       498\n",
      "        1320       0.76      0.74      0.75       648\n",
      "        1560       0.86      0.80      0.83      1015\n",
      "        1920       0.88      0.92      0.90       861\n",
      "        1940       0.66      0.86      0.75       161\n",
      "        2060       0.79      0.77      0.78       999\n",
      "        2220       0.68      0.87      0.76       165\n",
      "        2280       0.75      0.74      0.75       952\n",
      "        2403       0.76      0.61      0.68       955\n",
      "        2462       0.62      0.73      0.67       284\n",
      "        2522       0.90      0.89      0.90       998\n",
      "        2582       0.74      0.75      0.74       518\n",
      "        2583       0.98      0.97      0.98      2042\n",
      "        2585       0.76      0.83      0.79       499\n",
      "        2705       0.68      0.68      0.68       552\n",
      "        2905       0.98      0.99      0.99       174\n",
      "\n",
      "    accuracy                           0.79     16984\n",
      "   macro avg       0.75      0.78      0.76     16984\n",
      "weighted avg       0.79      0.79      0.79     16984\n",
      "\n",
      "Training Random Forest\n",
      "F1 Score (macro): 0.6243\n",
      "F1 Score (weighted): 0.6386\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          10       0.13      0.84      0.22       623\n",
      "          40       0.74      0.26      0.38       502\n",
      "          50       0.73      0.51      0.60       336\n",
      "          60       0.98      0.75      0.85       166\n",
      "        1140       0.73      0.48      0.58       534\n",
      "        1160       0.86      0.72      0.78       791\n",
      "        1180       0.55      0.35      0.43       153\n",
      "        1280       0.71      0.33      0.45       974\n",
      "        1281       0.58      0.33      0.42       414\n",
      "        1300       0.84      0.79      0.81      1009\n",
      "        1301       0.96      0.88      0.92       161\n",
      "        1302       0.82      0.58      0.68       498\n",
      "        1320       0.80      0.44      0.56       648\n",
      "        1560       0.76      0.59      0.67      1015\n",
      "        1920       0.88      0.81      0.84       861\n",
      "        1940       0.54      0.57      0.55       161\n",
      "        2060       0.60      0.74      0.66       999\n",
      "        2220       0.66      0.80      0.72       165\n",
      "        2280       0.75      0.45      0.56       952\n",
      "        2403       0.91      0.32      0.47       955\n",
      "        2462       0.44      0.80      0.57       284\n",
      "        2522       0.91      0.52      0.66       998\n",
      "        2582       0.54      0.67      0.60       518\n",
      "        2583       0.95      0.83      0.89      2042\n",
      "        2585       0.45      0.39      0.42       499\n",
      "        2705       0.58      0.58      0.58       552\n",
      "        2905       0.98      0.99      0.99       174\n",
      "\n",
      "    accuracy                           0.60     16984\n",
      "   macro avg       0.72      0.60      0.62     16984\n",
      "weighted avg       0.75      0.60      0.64     16984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## use smote for class balancing\n",
    "output1s = train_and_eval_basic(X_train_tfidf, y_train, X_test_tfidf, y_test, models, balance_classes=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8bce42",
   "metadata": {},
   "source": [
    "### VARIANT 2: using preprocessed tokens instead of raw text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "56897112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84916\n",
      "Training samples: 67932\n",
      "Test samples: 16984\n",
      "Vectorizing text...\n",
      "TF-IDF matrix shape: (67932, 9999)\n"
     ]
    }
   ],
   "source": [
    "X=df['comb_tokens'] ## !!!!!! the only difference here\n",
    "y=df['prdtypecode']\n",
    "\n",
    "print(y.size)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state, stratify=y)\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "\n",
    "print(\"Vectorizing text...\")\n",
    "X_train_tfidf_tok = vectorizer.fit_transform(X_train)  # fit + transform\n",
    "X_test_tfidf_tok = vectorizer.transform(X_test)        # transform only\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {X_train_tfidf_tok.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dcd420c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SMOTE for class balancing...\n",
      "Class distribution before SMOTE:\n",
      "  Class 0: 2493 samples\n",
      "  Class 5: 3162 samples\n",
      "  Class 10: 646 samples\n",
      "  Class 15: 642 samples\n",
      "  Class 20: 1137 samples\n",
      "  Class 25: 2209 samples\n",
      "Class distribution after SMOTE:\n",
      "  Class 0: 8167 samples\n",
      "  Class 5: 8167 samples\n",
      "  Class 10: 8167 samples\n",
      "  Class 15: 8167 samples\n",
      "  Class 20: 8167 samples\n",
      "  Class 25: 8167 samples\n",
      "Training Logistic Regression\n",
      "F1 Score (macro): 0.6727\n",
      "F1 Score (weighted): 0.7011\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          10       0.19      0.56      0.28       623\n",
      "          40       0.54      0.47      0.50       502\n",
      "          50       0.64      0.65      0.64       336\n",
      "          60       0.85      0.77      0.81       166\n",
      "        1140       0.60      0.53      0.56       534\n",
      "        1160       0.75      0.53      0.62       791\n",
      "        1180       0.27      0.39      0.32       153\n",
      "        1280       0.71      0.48      0.57       974\n",
      "        1281       0.50      0.49      0.49       414\n",
      "        1300       0.86      0.85      0.86      1009\n",
      "        1301       0.86      0.89      0.87       161\n",
      "        1302       0.66      0.76      0.71       498\n",
      "        1320       0.73      0.60      0.66       648\n",
      "        1560       0.79      0.77      0.78      1015\n",
      "        1920       0.86      0.91      0.88       861\n",
      "        1940       0.61      0.76      0.68       161\n",
      "        2060       0.78      0.73      0.75       999\n",
      "        2220       0.61      0.87      0.72       165\n",
      "        2280       0.58      0.46      0.51       952\n",
      "        2403       0.76      0.44      0.56       955\n",
      "        2462       0.53      0.56      0.54       284\n",
      "        2522       0.89      0.81      0.85       998\n",
      "        2582       0.70      0.72      0.71       518\n",
      "        2583       0.98      0.92      0.95      2042\n",
      "        2585       0.67      0.78      0.72       499\n",
      "        2705       0.59      0.65      0.62       552\n",
      "        2905       0.97      0.98      0.97       174\n",
      "\n",
      "    accuracy                           0.69     16984\n",
      "   macro avg       0.68      0.68      0.67     16984\n",
      "weighted avg       0.73      0.69      0.70     16984\n",
      "\n",
      "Training SGD Classifier\n",
      "F1 Score (macro): 0.6346\n",
      "F1 Score (weighted): 0.6594\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          10       0.17      0.55      0.26       623\n",
      "          40       0.55      0.38      0.45       502\n",
      "          50       0.62      0.60      0.61       336\n",
      "          60       0.89      0.75      0.81       166\n",
      "        1140       0.57      0.51      0.54       534\n",
      "        1160       0.77      0.51      0.62       791\n",
      "        1180       0.26      0.33      0.29       153\n",
      "        1280       0.64      0.41      0.50       974\n",
      "        1281       0.49      0.42      0.45       414\n",
      "        1300       0.80      0.84      0.82      1009\n",
      "        1301       0.79      0.89      0.84       161\n",
      "        1302       0.64      0.67      0.65       498\n",
      "        1320       0.72      0.50      0.59       648\n",
      "        1560       0.73      0.71      0.72      1015\n",
      "        1920       0.85      0.87      0.86       861\n",
      "        1940       0.55      0.73      0.63       161\n",
      "        2060       0.67      0.68      0.67       999\n",
      "        2220       0.64      0.85      0.73       165\n",
      "        2280       0.55      0.45      0.49       952\n",
      "        2403       0.82      0.39      0.53       955\n",
      "        2462       0.50      0.54      0.52       284\n",
      "        2522       0.87      0.75      0.81       998\n",
      "        2582       0.58      0.70      0.64       518\n",
      "        2583       0.97      0.88      0.92      2042\n",
      "        2585       0.61      0.69      0.65       499\n",
      "        2705       0.48      0.70      0.57       552\n",
      "        2905       0.96      0.98      0.97       174\n",
      "\n",
      "    accuracy                           0.65     16984\n",
      "   macro avg       0.66      0.64      0.63     16984\n",
      "weighted avg       0.70      0.65      0.66     16984\n",
      "\n",
      "Training Linear SVM\n",
      "F1 Score (macro): 0.6742\n",
      "F1 Score (weighted): 0.7052\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          10       0.19      0.56      0.28       623\n",
      "          40       0.52      0.49      0.50       502\n",
      "          50       0.68      0.64      0.66       336\n",
      "          60       0.79      0.78      0.79       166\n",
      "        1140       0.59      0.53      0.56       534\n",
      "        1160       0.76      0.54      0.63       791\n",
      "        1180       0.32      0.41      0.36       153\n",
      "        1280       0.71      0.51      0.59       974\n",
      "        1281       0.46      0.47      0.46       414\n",
      "        1300       0.89      0.84      0.87      1009\n",
      "        1301       0.79      0.88      0.83       161\n",
      "        1302       0.68      0.74      0.71       498\n",
      "        1320       0.73      0.62      0.67       648\n",
      "        1560       0.79      0.77      0.78      1015\n",
      "        1920       0.87      0.91      0.89       861\n",
      "        1940       0.62      0.76      0.68       161\n",
      "        2060       0.79      0.72      0.75       999\n",
      "        2220       0.63      0.86      0.73       165\n",
      "        2280       0.58      0.45      0.51       952\n",
      "        2403       0.72      0.45      0.56       955\n",
      "        2462       0.49      0.55      0.52       284\n",
      "        2522       0.88      0.82      0.85       998\n",
      "        2582       0.70      0.73      0.71       518\n",
      "        2583       0.98      0.94      0.96      2042\n",
      "        2585       0.70      0.79      0.74       499\n",
      "        2705       0.64      0.62      0.63       552\n",
      "        2905       0.96      0.98      0.97       174\n",
      "\n",
      "    accuracy                           0.69     16984\n",
      "   macro avg       0.68      0.68      0.67     16984\n",
      "weighted avg       0.73      0.69      0.71     16984\n",
      "\n",
      "Training Random Forest\n",
      "F1 Score (macro): 0.5269\n",
      "F1 Score (weighted): 0.5430\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          10       0.10      0.85      0.18       623\n",
      "          40       0.78      0.24      0.37       502\n",
      "          50       0.64      0.39      0.49       336\n",
      "          60       0.63      0.77      0.70       166\n",
      "        1140       0.70      0.28      0.40       534\n",
      "        1160       0.73      0.30      0.43       791\n",
      "        1180       0.62      0.19      0.29       153\n",
      "        1280       0.67      0.32      0.43       974\n",
      "        1281       0.49      0.35      0.41       414\n",
      "        1300       0.80      0.75      0.77      1009\n",
      "        1301       0.79      0.83      0.81       161\n",
      "        1302       0.75      0.43      0.55       498\n",
      "        1320       0.78      0.30      0.43       648\n",
      "        1560       0.68      0.53      0.59      1015\n",
      "        1920       0.88      0.76      0.81       861\n",
      "        1940       0.52      0.52      0.52       161\n",
      "        2060       0.57      0.68      0.62       999\n",
      "        2220       0.57      0.81      0.67       165\n",
      "        2280       0.31      0.11      0.17       952\n",
      "        2403       0.86      0.26      0.40       955\n",
      "        2462       0.38      0.52      0.44       284\n",
      "        2522       0.87      0.41      0.55       998\n",
      "        2582       0.46      0.60      0.52       518\n",
      "        2583       0.91      0.83      0.87      2042\n",
      "        2585       0.62      0.30      0.41       499\n",
      "        2705       0.52      0.43      0.47       552\n",
      "        2905       0.91      0.98      0.94       174\n",
      "\n",
      "    accuracy                           0.51     16984\n",
      "   macro avg       0.65      0.51      0.53     16984\n",
      "weighted avg       0.68      0.51      0.54     16984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output2 = train_and_eval_basic(X_train_tfidf_tok, y_train, X_test_tfidf_tok, y_test,  models, balance_classes=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b0de4e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(DATA_PATH + 'output1.npy', output1)\n",
    "np.save(DATA_PATH + 'output1s.npy', output1s)\n",
    "np.save(DATA_PATH + 'output2.npy', output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32530854",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-boot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
