{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cc41e17",
   "metadata": {},
   "source": [
    "### to be continued... ->> I moved the cnn-modeling to a cnn-images.py file\n",
    "- am more than happy to assist someone in transfering it back here if you guys prefer notebooks over scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2d331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System and utilities\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['OMP_NUM_THREADS'] = '12'\n",
    "os.environ['TF_NUM_INTEROP_THREADS'] = '4'\n",
    "os.environ['TF_NUM_INTRAOP_THREADS'] = '12'\n",
    "import time\n",
    "import argparse\n",
    "import json\n",
    "# Data manipulation and analysis\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "import sys\n",
    "sys.path.append('../src/functions')\n",
    "from training_report import get_training_report\n",
    "\n",
    "\n",
    "## KERAS\n",
    "from tensorflow.keras.utils import set_random_seed\n",
    "set_random_seed(66)  # Set random seed for reproducibility\n",
    "from tensorflow.config.threading import set_intra_op_parallelism_threads, set_inter_op_parallelism_threads\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization, Flatten\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts\n",
    "from tensorflow.keras.optimizers import Adam, AdamW\n",
    "from tensorflow.keras.metrics import F1Score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f4f399",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS          = 100\n",
    "BATCH_SIZE        = 64\n",
    "LR                = 0.01\n",
    "DATASET_PERC      = 0.9\n",
    "IMG_SIZE          = 224## 224 minimum\n",
    "CONV_FILTERS_1, CONV_FILTERS_2, CONV_FILTERS_3, CONV_FILTERS_4, CONV_FILTERS_5 = 16,16,32,64,128\n",
    "CONV_K_REG        = 0.0001\n",
    "DENSE_K_REG       = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ab249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA:\n",
    "OUT_PATH = '../misc/'\n",
    "PROC_DATA_PATH = '../processed_data/'\n",
    "df_full = pd.read_csv(PROC_DATA_PATH + 'X_train_multimodal.csv')\n",
    "df = df_full.head(int(df_full.shape[0]*DATASET_PERC))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bc8de1",
   "metadata": {},
   "source": [
    "##### 1. Create string labels for generators\n",
    "##### 2. Split data \n",
    "##### 3. Compute class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d5546d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create string labels for generators\n",
    "df['prdtypecode_str'] = df['prdtypecode'].astype(str)\n",
    "\n",
    "# Split data \n",
    "train_df, val_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.2, \n",
    "    random_state=66, \n",
    "    stratify=df['prdtypecode'])\n",
    "\n",
    "# Compute class weights \n",
    "y_train_for_weights = train_df['prdtypecode'].astype('category').cat.codes\n",
    "u_classes = np.unique(y_train_for_weights)\n",
    "class_weights = compute_class_weight('balanced', classes=u_classes, y=y_train_for_weights)\n",
    "class_weight_dict = {int(cls): round(float(weight), 3) for cls, weight in zip(u_classes, class_weights)}\n",
    "print(f'Class weights: {class_weight_dict}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c504b68d",
   "metadata": {},
   "source": [
    "##### Definition of data augmentation generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f49cb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.1,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4645ab5",
   "metadata": {},
   "source": [
    "##### Create data generators that load images from disk --> and apply data augmentation using the generators from the cell above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a187d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    x_col='imagepath',\n",
    "    y_col='prdtypecode_str',\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    seed=66)\n",
    "\n",
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "    val_df,\n",
    "    x_col='imagepath',\n",
    "    y_col='prdtypecode_str',\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    seed=66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa4fdf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4f981e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c55f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cf2b32f",
   "metadata": {},
   "source": [
    "# BUILDING THE CNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fde7223",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL SETUP:\n",
    "input_layer = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "# C1\n",
    "conv1 = Conv2D(filters=CONV_FILTERS_1, kernel_size=(3, 3), \n",
    "                activation='relu', kernel_regularizer=l2(CONV_K_REG))(input_layer)\n",
    "conv1 = BatchNormalization()(conv1)\n",
    "conv1 = Conv2D(filters=CONV_FILTERS_1, kernel_size=(3, 3), \n",
    "                activation='relu', kernel_regularizer=l2(CONV_K_REG))(conv1)\n",
    "conv1 = BatchNormalization()(conv1)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "pool1 = Dropout(0.25)(pool1)\n",
    "# C2\n",
    "conv2 = Conv2D(filters=CONV_FILTERS_2, kernel_size=(3, 3), \n",
    "                activation='relu', kernel_regularizer=l2(CONV_K_REG))(pool1)\n",
    "conv2 = BatchNormalization()(conv2)\n",
    "conv2 = Conv2D(filters=CONV_FILTERS_2, kernel_size=(3, 3), \n",
    "                activation='relu', kernel_regularizer=l2(CONV_K_REG))(conv2)\n",
    "conv2 = BatchNormalization()(conv2)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "pool2 = Dropout(0.25)(pool2)\n",
    "# C3\n",
    "conv3 = Conv2D(filters=CONV_FILTERS_3, kernel_size=(3, 3), \n",
    "                activation='relu', kernel_regularizer=l2(CONV_K_REG))(pool2)\n",
    "conv3 = BatchNormalization()(conv3)\n",
    "conv3 = Conv2D(filters=CONV_FILTERS_3, kernel_size=(3, 3), \n",
    "                activation='relu', kernel_regularizer=l2(CONV_K_REG))(conv3)\n",
    "conv3 = BatchNormalization()(conv3)\n",
    "pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "pool3 = Dropout(0.25)(pool3)\n",
    "# C4\n",
    "conv4 = Conv2D(filters=CONV_FILTERS_4, kernel_size=(3, 3), \n",
    "                activation='relu', kernel_regularizer=l2(CONV_K_REG))(pool3)\n",
    "conv4 = BatchNormalization()(conv4)\n",
    "conv4 = Conv2D(filters=CONV_FILTERS_4, kernel_size=(3, 3), \n",
    "                activation='relu', kernel_regularizer=l2(CONV_K_REG))(conv4)\n",
    "conv4 = BatchNormalization()(conv4)\n",
    "pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "pool4 = Dropout(0.25)(pool4)\n",
    "# # C5\n",
    "conv5 = Conv2D(filters=CONV_FILTERS_5, kernel_size=(3, 3), \n",
    "                activation='relu', kernel_regularizer=l2(CONV_K_REG))(pool4)\n",
    "conv5 = BatchNormalization()(conv5)\n",
    "conv5 = Conv2D(filters=CONV_FILTERS_5, kernel_size=(3, 3), \n",
    "                activation='relu', kernel_regularizer=l2(CONV_K_REG))(conv5)\n",
    "conv5 = BatchNormalization()(conv5)\n",
    "pool5 = MaxPooling2D(pool_size=(2, 2))(conv5)\n",
    "pool5 = Dropout(0.25)(pool5)\n",
    "\n",
    "# Global Average Pooling\n",
    "gap = GlobalAveragePooling2D()(pool5) ## change this to pool4 or pool5 if you want to use deeper layers\n",
    "\n",
    "# Dense layers\n",
    "# D1\n",
    "dense1 = Dense(512, activation='relu', kernel_regularizer=l2(DENSE_K_REG))(gap)\n",
    "dense1 = BatchNormalization()(dense1)\n",
    "dense1 = Dropout(0.4)(dense1)\n",
    "# D2\n",
    "dense2 = Dense(256, activation='relu', kernel_regularizer=l2(DENSE_K_REG))(dense1)\n",
    "dense2 = BatchNormalization()(dense2)\n",
    "dense2 = Dropout(0.3)(dense2)\n",
    "\n",
    "# D3\n",
    "dense3 = Dense(64, activation='relu', kernel_regularizer=l2(DENSE_K_REG))(dense2)\n",
    "dense3 = BatchNormalization()(dense3)\n",
    "dense3 = Dropout(0.3)(dense3)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(len(df['prdtypecode'].unique()), activation='softmax', kernel_regularizer=l2(0.001))(dense3)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf1bf20",
   "metadata": {},
   "source": [
    "##### Define the metric, early stopping, and two options for learning rate control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e90e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_metric = F1Score(average='macro', name='f1_score')\n",
    "early_stopping = EarlyStopping(monitor='val_f1_score', patience=7, restore_best_weights=True, verbose=1, mode='max')\n",
    "\n",
    "# Option 1: Reduce learning rate on plateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-7, verbose=1, mode='min')\n",
    "# Option 2: Cosine decay with restarts\n",
    "lr_schedule = CosineDecayRestarts(\n",
    "initial_learning_rate=LR,\n",
    "first_decay_steps=10,        # First cycle length\n",
    "t_mul=1.5,                   # Each cycle gets 1.5x longer\n",
    "m_mul=0.6,                   # LR drops 40% after each restart\n",
    "alpha=0.000001                 # Minimum LR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915f40ff",
   "metadata": {},
   "source": [
    "##### Seeing the full setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31454cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=AdamW(learning_rate=LR, weight_decay=0.01), loss='categorical_crossentropy',\n",
    "                metrics=['accuracy', f1_metric])\n",
    "print(model.summary())\n",
    "\n",
    "print('--'* 50)\n",
    "print(f'Using {DATASET_PERC*100}% of the dataset for training ({int(df_full.shape[0]*DATASET_PERC)} rows) .')\n",
    "print(f'Training set: {len(train_df)} samples, Validation set: {len(val_df)} samples')\n",
    "print(f'Starting Image CNN training with parameters:')\n",
    "print(f'------>number of epochs={N_EPOCHS}, batch_size={BATCH_SIZE}, initial learning_rate={LR}') \n",
    "print(f'------>image_size={IMG_SIZE}x{IMG_SIZE}')\n",
    "print(f'------>conv_filters=[{CONV_FILTERS_1, CONV_FILTERS_2,CONV_FILTERS_3,CONV_FILTERS_4,CONV_FILTERS_5}], conv_k_reg={CONV_K_REG}, dense_k_reg={DENSE_K_REG}')\n",
    "print('--'* 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3c69d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LONG RUNTIME WARNING !!!\n",
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7431fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=N_EPOCHS,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(val_generator),\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    class_weight=class_weight_dict,\n",
    "    verbose=1\n",
    ")\n",
    "print('Image CNN training completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e23a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f'image-cnn-epochs-{N_EPOCHS}-lr-{LR}_testing_valf1-{history.history[\"val_f1_score\"][-1]:.3f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeea9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting the training report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f2fe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61faece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = val_generator\n",
    "    \n",
    "# Use Keras built-in evaluate\n",
    "val_metrics = model.evaluate(validation_data, verbose=1)\n",
    "metric_names = model.metrics_names\n",
    "val_results = dict(zip(metric_names, val_metrics))\n",
    "\n",
    "# Generate predictions\n",
    "print(\"Generating predictions...\")\n",
    "validation_data.reset()\n",
    "y_pred = model.predict(validation_data, verbose=1)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = validation_data.classes\n",
    "\n",
    "\n",
    "class_names = df.prdtypecode_str.unique().tolist()  # Get class names from the DataFrame\n",
    "# Get class names and metadata\n",
    "if class_names is None:\n",
    "    class_names = list(validation_data.class_indices.keys())\n",
    "\n",
    "batch_size = validation_data.batch_size\n",
    "input_info = f\"Image Size: {validation_data.target_size}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9428cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate additional metrics\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_acc = history.history['val_accuracy'][-1]\n",
    "final_train_loss = history.history['loss'][-1]\n",
    "final_val_loss = history.history['val_loss'][-1]\n",
    "\n",
    "if 'f1_score' in history.history:\n",
    "    final_train_f1 = history.history['f1_score'][-1]\n",
    "    final_val_f1 = history.history['val_f1_score'][-1]\n",
    "else:\n",
    "    final_train_f1 = final_val_f1 = \"N/A\"\n",
    "\n",
    "# Generate classification report\n",
    "class_report = classification_report(y_true, y_pred_classes, \n",
    "                                    target_names=class_names, \n",
    "                                    output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6a9b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b189d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING CURVES + SUMMARY\n",
    "fig = plt.figure(figsize=(11, 8.5))\n",
    "\n",
    "# Create grid: 2x3 layout\n",
    "gs = fig.add_gridspec(2, 3, height_ratios=[1, 1], width_ratios=[1, 1, 0.8])\n",
    "\n",
    "# Training curves (top row)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.plot(history.history['loss'], label='Train', linewidth=2)\n",
    "ax1.plot(history.history['val_loss'], label='Val', linewidth=2)\n",
    "ax1.set_title('Loss', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.plot(history.history['accuracy'], label='Train', linewidth=2)\n",
    "ax2.plot(history.history['val_accuracy'], label='Val', linewidth=2)\n",
    "ax2.set_title('Accuracy', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# F1 and LR (bottom row)\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "if 'f1_score' in history.history:\n",
    "    ax3.plot(history.history['f1_score'], label='Train', linewidth=2)\n",
    "    ax3.plot(history.history['val_f1_score'], label='Val', linewidth=2)\n",
    "    ax3.set_title('F1 Score', fontweight='bold')\n",
    "    ax3.legend()\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'F1 not tracked', ha='center', va='center')\n",
    "    ax3.set_title('F1 Score (N/A)')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "if 'learning_rate' in history.history:\n",
    "    ax4.plot(history.history['learning_rate'], linewidth=2, color='red')\n",
    "    ax4.set_title('Learning Rate', fontweight='bold')\n",
    "    ax4.set_yscale('log')\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'LR not tracked', ha='center', va='center')\n",
    "    ax4.set_title('Learning Rate (N/A)')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Summary text (right side)\n",
    "ax_text = fig.add_subplot(gs[:, 2])\n",
    "ax_text.axis('off')\n",
    "\n",
    "# Format F1 scores safely\n",
    "train_f1_str = f\"{final_train_f1:.3f}\" if final_train_f1 != \"N/A\" else \"N/A\"\n",
    "val_f1_str = f\"{final_val_f1:.3f}\" if final_val_f1 != \"N/A\" else \"N/A\"\n",
    "\n",
    "summary_text = f\"\"\"{model_name}\n",
    "\n",
    "FINAL METRICS\n",
    "Train Acc: {final_train_acc:.3f}\n",
    "Val Acc:   {final_val_acc:.3f}\n",
    "Train F1:  {train_f1_str}\n",
    "Val F1:    {val_f1_str}\n",
    "\n",
    "Macro F1:  {class_report['macro avg']['f1-score']:.3f}\n",
    "Weight F1: {class_report['weighted avg']['f1-score']:.3f}\n",
    "\n",
    "MODEL INFO\n",
    "Params: {model.count_params():,}\n",
    "Epochs: {len(history.history['loss'])}\n",
    "Classes: {len(class_names)}\n",
    "{input_info}\"\"\"\n",
    "\n",
    "ax_text.text(0.05, 0.95, summary_text, transform=ax_text.transAxes, \n",
    "            fontsize=9, verticalalignment='top', fontfamily='monospace')\n",
    "\n",
    "plt.suptitle(f'Training Report - {model_name}', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6debc965",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PAGE 2: CONFUSION MATRIX + CLASS METRICS\n",
    "fig = plt.figure(figsize=(11, 8.5))\n",
    "gs = fig.add_gridspec(2, 2, height_ratios=[1.2, 1])\n",
    "\n",
    "# Confusion matrix (top, spanning both columns)\n",
    "ax_cm = fig.add_subplot(gs[0, :])\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names, ax=ax_cm)\n",
    "ax_cm.set_title('Confusion Matrix', fontweight='bold')\n",
    "ax_cm.set_xlabel('Predicted')\n",
    "ax_cm.set_ylabel('Actual')\n",
    "\n",
    "# Class metrics (bottom row)\n",
    "report_df = pd.DataFrame(class_report).transpose()\n",
    "class_metrics = report_df.loc[class_names, ['precision', 'recall', 'f1-score']]\n",
    "\n",
    "# Top 10 and bottom 10 classes by F1\n",
    "sorted_by_f1 = class_metrics.sort_values('f1-score')\n",
    "worst_10 = sorted_by_f1.head(10)\n",
    "best_10 = sorted_by_f1.tail(10)\n",
    "\n",
    "ax_worst = fig.add_subplot(gs[1, 0])\n",
    "worst_10['f1-score'].plot(kind='barh', ax=ax_worst, color='lightcoral')\n",
    "ax_worst.set_title('Bottom 10 Classes (F1 Score)', fontweight='bold')\n",
    "ax_worst.set_xlabel('F1 Score')\n",
    "\n",
    "ax_best = fig.add_subplot(gs[1, 1])\n",
    "best_10['f1-score'].plot(kind='barh', ax=ax_best, color='lightgreen')\n",
    "ax_best.set_title('Top 10 Classes (F1 Score)', fontweight='bold')\n",
    "ax_best.set_xlabel('F1 Score')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e812c226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2598eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    'Model Name': model_name,\n",
    "\n",
    "    'Total Parameters': model.count_params(),\n",
    "    'Training Epochs': len(history.history['loss']),\n",
    "    'Final Training Accuracy': f\"{final_train_acc:.4f}\",\n",
    "    'Final Validation Accuracy': f\"{final_val_acc:.4f}\",\n",
    "    'Final Training Loss': f\"{final_train_loss:.4f}\",\n",
    "    'Final Validation Loss': f\"{final_val_loss:.4f}\",\n",
    "    'Final Training F1': f\"{final_train_f1:.4f}\" if final_train_f1 != \"N/A\" else \"N/A\",\n",
    "    'Final Validation F1': f\"{final_val_f1:.4f}\" if final_val_f1 != \"N/A\" else \"N/A\",\n",
    "    'Validation Metrics': val_results,\n",
    "    'Macro Avg F1': f\"{class_report['macro avg']['f1-score']:.4f}\",\n",
    "    'Weighted Avg F1': f\"{class_report['weighted avg']['f1-score']:.4f}\",\n",
    "    'Number of Classes': len(class_names),\n",
    "    'Batch Size': batch_size,\n",
    "    'Input Info': input_info,\n",
    "}\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6187549",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-boot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
